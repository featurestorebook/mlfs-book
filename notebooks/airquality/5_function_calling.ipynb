{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f44e17-ed2d-47e7-887d-c07490a50f83",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Colab Users - Uncomment & Run the following 2 Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e353e1e-ace8-4ee5-9bef-86a9155e764b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment\n",
      "Added the following directory to the PYTHONPATH: c:\\Users\\Chris\\mlfs-book\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def is_google_colab() -> bool:\n",
    "    if \"google.colab\" in str(get_ipython()):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clone_repository() -> None:\n",
    "    !git clone https://github.com/featurestorebook/mlfs-book.git\n",
    "    %cd mlfs-book\n",
    "\n",
    "def install_dependencies() -> None:\n",
    "    !pip install --upgrade uv\n",
    "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
    "\n",
    "if is_google_colab():\n",
    "    clone_repository()\n",
    "    install_dependencies()\n",
    "    root_dir = str(Path().absolute())\n",
    "    print(\"Google Colab environment\")\n",
    "else:\n",
    "    root_dir = Path().absolute()\n",
    "    # Strip ~/notebooks/ccfraud from PYTHON_PATH if notebook started in one of these subdirectories\n",
    "    if root_dir.parts[-1:] == ('airquality',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    if root_dir.parts[-1:] == ('notebooks',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    root_dir = str(root_dir) \n",
    "    print(\"Local environment\")\n",
    "\n",
    "# Add the root directory to the `PYTHONPATH` to use the `recsys` Python module from the notebook.\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "print(f\"Added the following directory to the PYTHONPATH: {root_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f3f016",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "721ae546",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] Initieringen av en DLL-fil misslyckades. Error loading \"c:\\Users\\Chris\\anaconda3\\envs\\aq\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhopsworks\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmlfs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mairquality\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_chain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     load_model, \n\u001b[0;32m      6\u001b[0m     get_llm_chain, \n\u001b[0;32m      7\u001b[0m     generate_response, \n\u001b[0;32m      8\u001b[0m     generate_response_openai,\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chris\\mlfs-book\\mlfs\\airquality\\llm_chain.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig, AutoModel\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFacePipeline\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\aq\\Lib\\site-packages\\transformers\\__init__.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     30\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     is_pretty_midi_available,\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Note: the following symbols are deliberately exported with `as`\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# so that mypy, pylint or other static linters can recognize them,\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# given that they are not exported using `__all__` in this file.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\aq\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\aq\\Lib\\site-packages\\transformers\\utils\\__init__.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_docstring\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     25\u001b[0m     ClassAttrs,\n\u001b[0;32m     26\u001b[0m     ClassDocstring,\n\u001b[0;32m     27\u001b[0m     ImageProcessorArgs,\n\u001b[0;32m     28\u001b[0m     ModelArgs,\n\u001b[0;32m     29\u001b[0m     ModelOutputArgs,\n\u001b[0;32m     30\u001b[0m     auto_class_docstring,\n\u001b[0;32m     31\u001b[0m     auto_docstring,\n\u001b[0;32m     32\u001b[0m     get_args_doc_from_source,\n\u001b[0;32m     33\u001b[0m     parse_docstring,\n\u001b[0;32m     34\u001b[0m     set_min_indent,\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\aq\\Lib\\site-packages\\transformers\\utils\\auto_docstring.py:30\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mregex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     25\u001b[0m     MODELS_TO_PIPELINE,\n\u001b[0;32m     26\u001b[0m     PIPELINE_TASKS_TO_SAMPLE_DOCSTRINGS,\n\u001b[0;32m     27\u001b[0m     PT_SAMPLE_DOCSTRINGS,\n\u001b[0;32m     28\u001b[0m     _prepare_output_docstrings,\n\u001b[0;32m     29\u001b[0m )\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelOutput\n\u001b[0;32m     33\u001b[0m PATH_TO_TRANSFORMERS \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mresolve() \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m AUTODOC_FILES \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration_*.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodeling_*.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_extractor_*.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     44\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\aq\\Lib\\site-packages\\transformers\\utils\\generic.py:51\u001b[0m\n\u001b[0;32m     47\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# required for @can_return_tuple decorator to work with torchdynamo\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_debugging_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model_addition_debugger_context\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# vendored from distutils.util\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\aq\\Lib\\site-packages\\torch\\__init__.py:281\u001b[0m\n\u001b[0;32m    277\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    279\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 281\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\aq\\Lib\\site-packages\\torch\\__init__.py:264\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    260\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    261\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    263\u001b[0m     )\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1114] Initieringen av en DLL-fil misslyckades. Error loading \"c:\\Users\\Chris\\anaconda3\\envs\\aq\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import hopsworks\n",
    "from openai import OpenAI\n",
    "from mlfs.airquality.llm_chain import (\n",
    "    load_model, \n",
    "    get_llm_chain, \n",
    "    generate_response, \n",
    "    generate_response_openai,\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from mlfs import config\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b062cc0",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üîÆ Connect to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6340e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f2f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get_or_create the 'air_quality_fv' feature view\n",
    "feature_view = fs.get_feature_view(\n",
    "    name='air_quality_fv',\n",
    "    version=1\n",
    ")\n",
    "\n",
    "# Initialize batch scoring\n",
    "feature_view.init_batch_scoring(1)\n",
    "\n",
    "weather_fg = fs.get_feature_group(\n",
    "    name='weather',\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8002765b",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">ü™ù Retrieve AirQuality Model from Model Registry</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02695f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the model registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Retrieve the 'air_quality_xgboost_model' from the model registry\n",
    "retrieved_model = mr.get_model(\n",
    "    name=\"air_quality_xgboost_model\",\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "# Download the saved model artifacts  to a local directory\n",
    "saved_model_dir = retrieved_model.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the XGBoost regressor model and label encoder from the saved model directory\n",
    "# model_air_quality = joblib.load(saved_model_dir + \"/xgboost_regressor.pkl\")\n",
    "model_air_quality = XGBRegressor()\n",
    "\n",
    "model_air_quality.load_model(saved_model_dir + \"/model.json\")\n",
    "\n",
    "# Displaying the retrieved XGBoost regressor model\n",
    "model_air_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30142d",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>‚¨áÔ∏è LLM Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911a86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the LLM and its corresponding tokenizer.\n",
    "model_llm, tokenizer = load_model(model_id=\"imiraoui/OpenHermes-2.5-Mistral-7B-sharded\")\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"The code execution took {duration} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e329285",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>‚õìÔ∏è LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf5ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Create and configure a language model chain.\n",
    "llm_chain = get_llm_chain(\n",
    "    model_llm,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"The code execution took {duration} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ded5c",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>üß¨ Domain-specific Evaluation Harness\n",
    "\n",
    "**Systematic evaluations** that can run automatically in CI/CD pipelines are key to evaluating models/RAG. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58181b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION7 = \"Hi!\"\n",
    "\n",
    "response7 = generate_response(\n",
    "    QUESTION7,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec32e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Who are you?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION1 = \"What was the average air quality from 2024-01-10 till 2024-01-14?\"\n",
    "\n",
    "response1 = generate_response(\n",
    "    QUESTION1, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d01dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION11 = \"When and what was the air quality like last week?\"\n",
    "\n",
    "response11 = generate_response(\n",
    "    QUESTION11, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION12 = \"When and what was the minimum air quality from 2024-01-10 till 2024-01-14?\"\n",
    "\n",
    "response12 = generate_response(\n",
    "    QUESTION12, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION2a = \"What was the air quality like last week?\"\n",
    "\n",
    "response2 = generate_response(\n",
    "    QUESTION2a,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e6bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION2 = \"What was the air quality like yesterday?\"\n",
    "\n",
    "response2 = generate_response(\n",
    "    QUESTION2,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed349483",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION3 = \"What will the air quality be like next Tuesday?\"\n",
    "\n",
    "response3 = generate_response(\n",
    "    QUESTION3, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6825c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION4 = \"What will the air quality be like the day after tomorrow?\"\n",
    "\n",
    "response4 = generate_response(\n",
    "    QUESTION4, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION5 = \"What will the air quality be like this Sunday?\"\n",
    "\n",
    "response5 = generate_response(\n",
    "    QUESTION5, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee271416",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION7 = \"What will the air quality be like for the rest of the week?\"\n",
    "\n",
    "response7 = generate_response(\n",
    "    QUESTION7, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeeb4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Will the air quality be safe or not for the next week?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION7, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Is tomorrow's air quality level dangerous?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb26726",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Can you please explain different PM2_5 air quality levels?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a463f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai --quiet\n",
    "# !pip install gradio==3.40.1 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4aebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from functions.llm_chain import load_model, get_llm_chain, generate_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ASR pipeline\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "\n",
    "def transcribe(audio):\n",
    "    sr, y = audio\n",
    "    y = y.astype(np.float32)\n",
    "    if y.ndim > 1 and y.shape[1] > 1:\n",
    "        y = np.mean(y, axis=1)\n",
    "    y /= np.max(np.abs(y))\n",
    "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n",
    "\n",
    "def generate_query_response(user_query, method, openai_api_key=None):\n",
    "    if method == 'Hermes LLM':        \n",
    "        response = generate_response(\n",
    "            user_query,\n",
    "            feature_view,\n",
    "            weather_fg,\n",
    "            model_air_quality,\n",
    "            model_llm,\n",
    "            tokenizer,\n",
    "            llm_chain,\n",
    "            verbose=False,\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    elif method == 'OpenAI API' and openai_api_key:\n",
    "        client = OpenAI(\n",
    "            api_key=openai_api_key\n",
    "        )\n",
    "        \n",
    "        response = generate_response_openai(   \n",
    "            user_query,\n",
    "            feature_view,\n",
    "            weather_fg,\n",
    "            model_air_quality,\n",
    "            client=client,\n",
    "            verbose=True,\n",
    "        )\n",
    "        return response\n",
    "        \n",
    "    else:\n",
    "        return \"Invalid method or missing API key.\"\n",
    "\n",
    "def handle_input(text_input=None, audio_input=None, method='Hermes LLM', openai_api_key=\"\"):\n",
    "    if audio_input is not None:\n",
    "        user_query = transcribe(audio_input)\n",
    "    else:\n",
    "        user_query = text_input\n",
    "    \n",
    "    # Check if OpenAI API key is required but not provided\n",
    "    if method == 'OpenAI API' and not openai_api_key.strip():\n",
    "        return \"OpenAI API key is required for this method.\"\n",
    "\n",
    "    if user_query:\n",
    "        return generate_query_response(user_query, method, openai_api_key)\n",
    "    else:\n",
    "        return \"Please provide input either via text or voice.\"\n",
    "    \n",
    "\n",
    "# Setting up the Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=handle_input,\n",
    "    inputs=[\n",
    "        gr.Textbox(placeholder=\"Type here or use voice input...\"), \n",
    "        gr.Audio(), \n",
    "        gr.Radio([\"Hermes LLM\", \"OpenAI API\"], label=\"Choose the response generation method\"),\n",
    "        gr.Textbox(label=\"Enter your OpenAI API key (only if you selected OpenAI API):\", type=\"password\")  # Removed `optional=True`\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"üå§Ô∏è AirQuality AI Assistant üí¨\",\n",
    "    description=\"Ask your questions about air quality or use your voice to interact. Select the response generation method and provide an OpenAI API key if necessary.\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afa7c6a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
