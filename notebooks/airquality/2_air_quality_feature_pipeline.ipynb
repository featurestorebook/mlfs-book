{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4447764c-218b-441a-ab97-4df4062960d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment\n",
      "Added the following directory to the PYTHONPATH: /Users/hayleychang/Desktop/mlfs-book\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def is_google_colab() -> bool:\n",
    "    if \"google.colab\" in str(get_ipython()):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clone_repository() -> None:\n",
    "    !git clone https://github.com/featurestorebook/mlfs-book.git\n",
    "    %cd mlfs-book\n",
    "\n",
    "def install_dependencies() -> None:\n",
    "    !pip install --upgrade uv\n",
    "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
    "\n",
    "if is_google_colab():\n",
    "    clone_repository()\n",
    "    install_dependencies()\n",
    "    root_dir = str(Path().absolute())\n",
    "    print(\"Google Colab environment\")\n",
    "else:\n",
    "    root_dir = Path().absolute()\n",
    "    # Strip ~/notebooks/ccfraud from PYTHON_PATH if notebook started in one of these subdirectories\n",
    "    if root_dir.parts[-1:] == ('airquality',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    if root_dir.parts[-1:] == ('notebooks',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    root_dir = str(root_dir) \n",
    "    print(\"Local environment\")\n",
    "\n",
    "# Add the root directory to the `PYTHONPATH` to use the `recsys` Python module from the notebook.\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "print(f\"Added the following directory to the PYTHONPATH: {root_dir}\")\n",
    "    \n",
    "# Set the environment variables from the file <root_dir>/.env\n",
    "# from mlfs import config\n",
    "# settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e46aad",
   "metadata": {},
   "source": [
    "<span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 02: Daily Feature Pipeline for Air Quality (aqicn.org) and weather (openmeteo)</span>\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into the following sections:\n",
    "1. Download and Parse Data\n",
    "2. Feature Group Insertion\n",
    "\n",
    "\n",
    "__This notebook should be scheduled to run daily__\n",
    "\n",
    "In the book, we use a GitHub Action stored here:\n",
    "[.github/workflows/air-quality-daily.yml](https://github.com/featurestorebook/mlfs-book/blob/main/.github/workflows/air-quality-daily.yml)\n",
    "\n",
    "However, you are free to use any Python Orchestration tool to schedule this program to run daily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe638c6",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de2e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import hopsworks\n",
    "from mlfs.airquality import util\n",
    "from mlfs import config\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6081d1",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üåç Get the Sensor URL, Country, City, Street names from Hopsworks </span>\n",
    "\n",
    "__Update the values in the cell below.__\n",
    "\n",
    "__These should be the same values as in notebook 1 - the feature backfill notebook__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70cd57d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-14 17:51:20,847 INFO: Initializing external client\n",
      "2025-11-14 17:51:20,848 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-14 17:51:22,616 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1286345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"country\": \"united-states-of-america\", \"city\": \"los-angeles\", \"street\": \"arroyo-seco-museum-science-magnet-span-4322\", \"aqicn_url\": \"https://api.waqi.info/feed/A399199\", \"latitude\": 34.05, \"longitude\": -118.24}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = hopsworks.login(engine=\"python\")\n",
    "fs = project.get_feature_store() \n",
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "# This line will fail if you have not registered the AQICN_API_KEY as a secret in Hopsworks\n",
    "AQICN_API_KEY = secrets.get_secret(\"AQICN_API_KEY\").value\n",
    "location_str = secrets.get_secret(\"SENSOR_LOCATION_JSON\").value\n",
    "location = json.loads(location_str)\n",
    "\n",
    "country=location['country']\n",
    "city=location['city']\n",
    "street=location['street']\n",
    "aqicn_url=location['aqicn_url']\n",
    "latitude=location['latitude']\n",
    "longitude=location['longitude']\n",
    "\n",
    "today = datetime.date.today()\n",
    "\n",
    "location_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf9289",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> üîÆ Get references to the Feature Groups </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66f5d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature groups\n",
    "air_quality_fg = fs.get_feature_group(\n",
    "    name='air_quality',\n",
    "    version=1,\n",
    ")\n",
    "weather_fg = fs.get_feature_group(\n",
    "    name='weather',\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "lagged_pm25 = fs.get_or_create_feature_group(\n",
    "    name='lagged_pm25',\n",
    "    description='Lagged PM2.5 measurements',\n",
    "    version=1,\n",
    "    primary_key=['city', 'date'],\n",
    "    event_time=\"date\",\n",
    "    expectation_suite=None\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b6ce8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7ffa41",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üå´ Retrieve Today's Air Quality data (PM2.5) from the AQI API</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f681af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pm25</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>street</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35.0</td>\n",
       "      <td>united-states-of-america</td>\n",
       "      <td>los-angeles</td>\n",
       "      <td>arroyo-seco-museum-science-magnet-span-4322</td>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>https://api.waqi.info/feed/A399199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pm25                   country         city  \\\n",
       "0  35.0  united-states-of-america  los-angeles   \n",
       "\n",
       "                                        street       date  \\\n",
       "0  arroyo-seco-museum-science-magnet-span-4322 2025-11-14   \n",
       "\n",
       "                                  url  \n",
       "0  https://api.waqi.info/feed/A399199  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "aq_today_df = util.get_pm25(aqicn_url, country, city, street, today, AQICN_API_KEY)\n",
    "aq_today_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9e24eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   pm25     1 non-null      float32       \n",
      " 1   country  1 non-null      object        \n",
      " 2   city     1 non-null      object        \n",
      " 3   street   1 non-null      object        \n",
      " 4   date     1 non-null      datetime64[ns]\n",
      " 5   url      1 non-null      object        \n",
      "dtypes: datetime64[ns](1), float32(1), object(4)\n",
      "memory usage: 176.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "aq_today_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af845ab6",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üå¶ Get Weather Forecast data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2ecb3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates 34.0¬∞N -118.25¬∞E\n",
      "Elevation 87.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.73s) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temperature_2m_mean</th>\n",
       "      <th>precipitation_sum</th>\n",
       "      <th>wind_speed_10m_max</th>\n",
       "      <th>wind_direction_10m_dominant</th>\n",
       "      <th>city</th>\n",
       "      <th>wind_speed_10m_max_squared</th>\n",
       "      <th>wind_u</th>\n",
       "      <th>wind_v</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>day_of_week_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>precipitation_binary</th>\n",
       "      <th>temp_wind_interaction</th>\n",
       "      <th>precip_wind_interaction</th>\n",
       "      <th>precip_wind_u</th>\n",
       "      <th>temperature_30d_avg</th>\n",
       "      <th>temperature_anomaly</th>\n",
       "      <th>temp_anomaly_wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>15.90</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.739833</td>\n",
       "      <td>103.570457</td>\n",
       "      <td>los-angeles</td>\n",
       "      <td>115.344009</td>\n",
       "      <td>10.440000</td>\n",
       "      <td>-2.520004e+00</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1</td>\n",
       "      <td>170.763336</td>\n",
       "      <td>3.221950</td>\n",
       "      <td>3.132000</td>\n",
       "      <td>18.248003</td>\n",
       "      <td>-2.348003</td>\n",
       "      <td>-25.217164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-15</td>\n",
       "      <td>15.55</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>los-angeles</td>\n",
       "      <td>1.166400</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>-4.720830e-08</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1</td>\n",
       "      <td>16.794001</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>18.248003</td>\n",
       "      <td>-2.698003</td>\n",
       "      <td>-2.913843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-16</td>\n",
       "      <td>11.15</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.075841</td>\n",
       "      <td>110.556129</td>\n",
       "      <td>los-angeles</td>\n",
       "      <td>9.460800</td>\n",
       "      <td>2.879999</td>\n",
       "      <td>-1.080004e+00</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1</td>\n",
       "      <td>34.295631</td>\n",
       "      <td>0.615168</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>18.248003</td>\n",
       "      <td>-7.098003</td>\n",
       "      <td>-21.832333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>13.15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5.315336</td>\n",
       "      <td>118.300667</td>\n",
       "      <td>los-angeles</td>\n",
       "      <td>28.252800</td>\n",
       "      <td>4.680004</td>\n",
       "      <td>-2.519993e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1</td>\n",
       "      <td>69.896667</td>\n",
       "      <td>1.594601</td>\n",
       "      <td>1.404001</td>\n",
       "      <td>18.248003</td>\n",
       "      <td>-5.098003</td>\n",
       "      <td>-27.097602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-18</td>\n",
       "      <td>8.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.259938</td>\n",
       "      <td>83.659904</td>\n",
       "      <td>los-angeles</td>\n",
       "      <td>10.627199</td>\n",
       "      <td>3.240000</td>\n",
       "      <td>3.599944e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0</td>\n",
       "      <td>26.731495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.248003</td>\n",
       "      <td>-10.048003</td>\n",
       "      <td>-32.755872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-19</td>\n",
       "      <td>6.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.620839</td>\n",
       "      <td>344.054535</td>\n",
       "      <td>los-angeles</td>\n",
       "      <td>6.868799</td>\n",
       "      <td>-0.720003</td>\n",
       "      <td>2.519999e+00</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0</td>\n",
       "      <td>15.987120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>18.248003</td>\n",
       "      <td>-12.148003</td>\n",
       "      <td>-31.837963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-20</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.235910</td>\n",
       "      <td>95.710503</td>\n",
       "      <td>los-angeles</td>\n",
       "      <td>52.358398</td>\n",
       "      <td>7.200001</td>\n",
       "      <td>-7.199889e-01</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0</td>\n",
       "      <td>79.595016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.248003</td>\n",
       "      <td>-7.248003</td>\n",
       "      <td>-52.445900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  temperature_2m_mean  precipitation_sum  wind_speed_10m_max  \\\n",
       "0 2025-11-14                15.90                0.3           10.739833   \n",
       "1 2025-11-15                15.55                2.5            1.080000   \n",
       "2 2025-11-16                11.15                0.2            3.075841   \n",
       "3 2025-11-17                13.15                0.3            5.315336   \n",
       "4 2025-11-18                 8.20                0.0            3.259938   \n",
       "5 2025-11-19                 6.10                0.0            2.620839   \n",
       "6 2025-11-20                11.00                0.0            7.235910   \n",
       "\n",
       "   wind_direction_10m_dominant         city  wind_speed_10m_max_squared  \\\n",
       "0                   103.570457  los-angeles                  115.344009   \n",
       "1                    90.000000  los-angeles                    1.166400   \n",
       "2                   110.556129  los-angeles                    9.460800   \n",
       "3                   118.300667  los-angeles                   28.252800   \n",
       "4                    83.659904  los-angeles                   10.627199   \n",
       "5                   344.054535  los-angeles                    6.868799   \n",
       "6                    95.710503  los-angeles                   52.358398   \n",
       "\n",
       "      wind_u        wind_v  day_of_week  ...  day_of_week_cos  month_sin  \\\n",
       "0  10.440000 -2.520004e+00            4  ...        -0.900969       -0.5   \n",
       "1   1.080000 -4.720830e-08            5  ...        -0.222521       -0.5   \n",
       "2   2.879999 -1.080004e+00            6  ...         0.623490       -0.5   \n",
       "3   4.680004 -2.519993e+00            0  ...         1.000000       -0.5   \n",
       "4   3.240000  3.599944e-01            1  ...         0.623490       -0.5   \n",
       "5  -0.720003  2.519999e+00            2  ...        -0.222521       -0.5   \n",
       "6   7.200001 -7.199889e-01            3  ...        -0.900969       -0.5   \n",
       "\n",
       "   month_cos  precipitation_binary  temp_wind_interaction  \\\n",
       "0   0.866025                     1             170.763336   \n",
       "1   0.866025                     1              16.794001   \n",
       "2   0.866025                     1              34.295631   \n",
       "3   0.866025                     1              69.896667   \n",
       "4   0.866025                     0              26.731495   \n",
       "5   0.866025                     0              15.987120   \n",
       "6   0.866025                     0              79.595016   \n",
       "\n",
       "   precip_wind_interaction  precip_wind_u  temperature_30d_avg  \\\n",
       "0                 3.221950       3.132000            18.248003   \n",
       "1                 2.700000       2.700000            18.248003   \n",
       "2                 0.615168       0.576000            18.248003   \n",
       "3                 1.594601       1.404001            18.248003   \n",
       "4                 0.000000       0.000000            18.248003   \n",
       "5                 0.000000      -0.000000            18.248003   \n",
       "6                 0.000000       0.000000            18.248003   \n",
       "\n",
       "   temperature_anomaly  temp_anomaly_wind_speed  \n",
       "0            -2.348003               -25.217164  \n",
       "1            -2.698003                -2.913843  \n",
       "2            -7.098003               -21.832333  \n",
       "3            -5.098003               -27.097602  \n",
       "4           -10.048003               -32.755872  \n",
       "5           -12.148003               -31.837963  \n",
       "6            -7.248003               -52.445900  \n",
       "\n",
       "[7 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_df = util.get_hourly_weather_forecast(city, latitude, longitude)\n",
    "hourly_df = hourly_df.set_index('date')\n",
    "hourly_df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# We will only make 1 daily prediction, so we will replace the hourly forecasts with a single daily forecast\n",
    "# We only want the daily weather data, so only get weather at 12:00\n",
    "daily_df = hourly_df.between_time('11:59', '12:01')\n",
    "daily_df = daily_df.reset_index()\n",
    "daily_df['date'] = pd.to_datetime(daily_df['date']).dt.date\n",
    "daily_df['date'] = pd.to_datetime(daily_df['date'])\n",
    "daily_df['city'] = city\n",
    "\n",
    "# Wind speed squared\n",
    "daily_df['wind_speed_10m_max_squared'] = daily_df['wind_speed_10m_max'] ** 2\n",
    "\n",
    "# Wind speed directions\n",
    "daily_df['wind_u'] = daily_df['wind_speed_10m_max'] * np.sin(np.radians(daily_df['wind_direction_10m_dominant']))\n",
    "daily_df['wind_v'] = daily_df['wind_speed_10m_max'] * np.cos(np.radians(daily_df['wind_direction_10m_dominant']))\n",
    "\n",
    "# Temporal signals\n",
    "daily_df['day_of_week'] = daily_df['date'].dt.dayofweek          # 0=Mon\n",
    "daily_df['month'] = daily_df['date'].dt.month\n",
    "daily_df['is_weekend'] = (daily_df['day_of_week'] >= 5).astype(int)\n",
    "daily_df['day_of_year'] = daily_df['date'].dt.dayofyear\n",
    "\n",
    "# Cyclical encoding for day/month so models ‚Äúfeel‚Äù seasonality\n",
    "daily_df['day_of_week_sin'] = np.sin(2 * np.pi * daily_df['day_of_week'] / 7)\n",
    "daily_df['day_of_week_cos'] = np.cos(2 * np.pi * daily_df['day_of_week'] / 7)\n",
    "daily_df['month_sin'] = np.sin(2 * np.pi * daily_df['month'] / 12)\n",
    "daily_df['month_cos'] = np.cos(2 * np.pi * daily_df['month'] / 12)\n",
    "\n",
    "# Weather-derived interactions\n",
    "daily_df['precipitation_binary'] = (daily_df['precipitation_sum'] > 0).astype(int)\n",
    "daily_df['temp_wind_interaction'] = daily_df['temperature_2m_mean'] * daily_df['wind_speed_10m_max']\n",
    "daily_df['precip_wind_interaction'] = daily_df['precipitation_sum'] * daily_df['wind_speed_10m_max']\n",
    "\n",
    "daily_df[\"precip_wind_u\"] = daily_df[\"precipitation_sum\"] * daily_df[\"wind_u\"]\n",
    "\n",
    "# Anomaly weather detection\n",
    "# get past 30 days of weather data\n",
    "history = weather_fg.filter(\n",
    "    (weather_fg.city == city) &\n",
    "    (weather_fg.date >= today - timedelta(days=30)) &\n",
    "    (weather_fg.date < today)\n",
    ").read()\n",
    "\n",
    "# calculate avg temp of past 30 days\n",
    "if not history.empty:\n",
    "    temp_30d_avg = history['temperature_2m_mean'].mean()\n",
    "else:\n",
    "    temp_30d_avg = daily_df['temperature_2m_mean'].iloc[0]\n",
    "daily_df[\"temperature_30d_avg\"] = temp_30d_avg.astype('float64')\n",
    "# diff between current temp and avg temp of past 30 days\n",
    "daily_df['temperature_anomaly'] = (daily_df['temperature_2m_mean'] - temp_30d_avg ).astype('float64')\n",
    "daily_df[\"temp_anomaly_wind_speed\"] = (daily_df[\"temperature_anomaly\"] * daily_df[\"wind_speed_10m_max\"]).astype('float64')\n",
    "\n",
    "daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c563109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 24 columns):\n",
      " #   Column                       Non-Null Count  Dtype         \n",
      "---  ------                       --------------  -----         \n",
      " 0   date                         7 non-null      datetime64[ns]\n",
      " 1   temperature_2m_mean          7 non-null      float32       \n",
      " 2   precipitation_sum            7 non-null      float32       \n",
      " 3   wind_speed_10m_max           7 non-null      float32       \n",
      " 4   wind_direction_10m_dominant  7 non-null      float32       \n",
      " 5   city                         7 non-null      object        \n",
      " 6   wind_speed_10m_max_squared   7 non-null      float32       \n",
      " 7   wind_u                       7 non-null      float32       \n",
      " 8   wind_v                       7 non-null      float32       \n",
      " 9   day_of_week                  7 non-null      int32         \n",
      " 10  month                        7 non-null      int32         \n",
      " 11  is_weekend                   7 non-null      int64         \n",
      " 12  day_of_year                  7 non-null      int32         \n",
      " 13  day_of_week_sin              7 non-null      float64       \n",
      " 14  day_of_week_cos              7 non-null      float64       \n",
      " 15  month_sin                    7 non-null      float64       \n",
      " 16  month_cos                    7 non-null      float64       \n",
      " 17  precipitation_binary         7 non-null      int64         \n",
      " 18  temp_wind_interaction        7 non-null      float32       \n",
      " 19  precip_wind_interaction      7 non-null      float32       \n",
      " 20  precip_wind_u                7 non-null      float32       \n",
      " 21  temperature_30d_avg          7 non-null      float64       \n",
      " 22  temperature_anomaly          7 non-null      float64       \n",
      " 23  temp_anomaly_wind_speed      7 non-null      float64       \n",
      "dtypes: datetime64[ns](1), float32(10), float64(7), int32(3), int64(2), object(1)\n",
      "memory usage: 1.1+ KB\n"
     ]
    }
   ],
   "source": [
    "daily_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f5008",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:#ff5f27;\">‚¨ÜÔ∏è Uploading new data to the Feature Store</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a9de5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-14 17:51:31,249 INFO: \t1 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1286345/fs/1273967/fg/1717594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1/1 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: air_quality_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1286345/jobs/named/air_quality_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('air_quality_1_offline_fg_materialization', 'SPARK'),\n",
       " {\n",
       "   \"success\": true,\n",
       "   \"results\": [\n",
       "     {\n",
       "       \"success\": true,\n",
       "       \"expectation_config\": {\n",
       "         \"expectation_type\": \"expect_column_min_to_be_between\",\n",
       "         \"kwargs\": {\n",
       "           \"column\": \"pm25\",\n",
       "           \"min_value\": -0.1,\n",
       "           \"max_value\": 500.0,\n",
       "           \"strict_min\": true\n",
       "         },\n",
       "         \"meta\": {\n",
       "           \"expectationId\": 757764\n",
       "         }\n",
       "       },\n",
       "       \"result\": {\n",
       "         \"observed_value\": 35.0,\n",
       "         \"element_count\": 1,\n",
       "         \"missing_count\": null,\n",
       "         \"missing_percent\": null\n",
       "       },\n",
       "       \"meta\": {\n",
       "         \"ingestionResult\": \"INGESTED\",\n",
       "         \"validationTime\": \"2025-11-14T04:51:31.000248Z\"\n",
       "       },\n",
       "       \"exception_info\": {\n",
       "         \"raised_exception\": false,\n",
       "         \"exception_message\": null,\n",
       "         \"exception_traceback\": null\n",
       "       }\n",
       "     }\n",
       "   ],\n",
       "   \"evaluation_parameters\": {},\n",
       "   \"statistics\": {\n",
       "     \"evaluated_expectations\": 1,\n",
       "     \"successful_expectations\": 1,\n",
       "     \"unsuccessful_expectations\": 0,\n",
       "     \"success_percent\": 100.0\n",
       "   },\n",
       "   \"meta\": {\n",
       "     \"great_expectations_version\": \"0.18.12\",\n",
       "     \"expectation_suite_name\": \"aq_expectation_suite\",\n",
       "     \"run_id\": {\n",
       "       \"run_name\": null,\n",
       "       \"run_time\": \"2025-11-14T17:51:31.249021+01:00\"\n",
       "     },\n",
       "     \"batch_kwargs\": {\n",
       "       \"ge_batch_id\": \"2bbd3ba8-c17a-11f0-8c5d-acde48001122\"\n",
       "     },\n",
       "     \"batch_markers\": {},\n",
       "     \"batch_parameters\": {},\n",
       "     \"validation_time\": \"20251114T165131.248929Z\",\n",
       "     \"expectation_suite_meta\": {\n",
       "       \"great_expectations_version\": \"0.18.12\"\n",
       "     }\n",
       "   }\n",
       " })"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert new data\n",
    "air_quality_fg.insert(aq_today_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d491b0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-14 17:52:10,532 INFO: \t2 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1286345/fs/1273967/fg/1717595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 7/7 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: weather_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1286345/jobs/named/weather_1_offline_fg_materialization/executions\n",
      "2025-11-14 17:52:27,956 INFO: Waiting for execution to finish. Current state: INITIALIZING. Final status: UNDEFINED\n",
      "2025-11-14 17:52:31,942 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2025-11-14 17:52:38,716 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: KILLED\n",
      "2025-11-14 17:52:38,892 INFO: Waiting for log aggregation to finish.\n"
     ]
    },
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/1286345/jobs/weather_1_offline_fg_materialization/executions/3828158). Server response: \nHTTP code: 404, HTTP reason: Not Found, body: b'{\"errorCode\":130009,\"usrMsg\":\"jobId:weather_1_offline_fg_materialization\",\"errorMsg\":\"Job not found.\"}', error code: 130009, error msg: Job not found., user msg: jobId:weather_1_offline_fg_materialization",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRestAPIError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Insert new data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mweather_fg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdaily_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mlfs/lib/python3.11/site-packages/hsfs/feature_group.py:3153\u001b[39m, in \u001b[36mFeatureGroup.insert\u001b[39m\u001b[34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[39m\n\u001b[32m   3150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._id \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._offline_backfill_every_hr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3151\u001b[39m     write_options[\u001b[33m\"\u001b[39m\u001b[33moffline_backfill_every_hr\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._offline_backfill_every_hr\n\u001b[32m-> \u001b[39m\u001b[32m3153\u001b[39m job, ge_report = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feature_group_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3154\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3156\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3157\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msave_report\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3163\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine.get_type().startswith(\u001b[33m\"\u001b[39m\u001b[33mspark\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n\u001b[32m   3166\u001b[39m     \u001b[38;5;66;03m# Also, only compute statistics if stream is False.\u001b[39;00m\n\u001b[32m   3167\u001b[39m     \u001b[38;5;66;03m# if True, the backfill job has not been triggered and the data has not been inserted (it's in Kafka)\u001b[39;00m\n\u001b[32m   3168\u001b[39m     \u001b[38;5;28mself\u001b[39m.compute_statistics()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mlfs/lib/python3.11/site-packages/hsfs/core/feature_group_engine.py:245\u001b[39m, in \u001b[36mFeatureGroupEngine.insert\u001b[39m\u001b[34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options, transformation_context, transform)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overwrite:\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m._feature_group_api.delete_content(feature_group)\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m     \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbulk_insert\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43monline_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43monline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    254\u001b[39m     ge_report,\n\u001b[32m    255\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mlfs/lib/python3.11/site-packages/hsfs/engine/python.py:819\u001b[39m, in \u001b[36mEngine.save_dataframe\u001b[39m\u001b[34m(self, feature_group, dataframe, operation, online_enabled, storage, offline_write_options, online_write_options, validation_id)\u001b[39m\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_dataframe\u001b[39m(\n\u001b[32m    805\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    806\u001b[39m     feature_group: FeatureGroup,\n\u001b[32m   (...)\u001b[39m\u001b[32m    813\u001b[39m     validation_id: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    814\u001b[39m ) -> Optional[job.Job]:\n\u001b[32m    815\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    816\u001b[39m         \u001b[38;5;28mhasattr\u001b[39m(feature_group, \u001b[33m\"\u001b[39m\u001b[33mEXTERNAL_FEATURE_GROUP\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    817\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m feature_group.online_enabled\n\u001b[32m    818\u001b[39m     ) \u001b[38;5;129;01mor\u001b[39;00m feature_group.stream:\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_write_dataframe_kafka\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    822\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    823\u001b[39m         \u001b[38;5;66;03m# for backwards compatibility\u001b[39;00m\n\u001b[32m    824\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.legacy_save_dataframe(\n\u001b[32m    825\u001b[39m             feature_group,\n\u001b[32m    826\u001b[39m             dataframe,\n\u001b[32m   (...)\u001b[39m\u001b[32m    832\u001b[39m             validation_id,\n\u001b[32m    833\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mlfs/lib/python3.11/site-packages/hsfs/engine/python.py:1605\u001b[39m, in \u001b[36mEngine._write_dataframe_kafka\u001b[39m\u001b[34m(self, feature_group, dataframe, offline_write_options)\u001b[39m\n\u001b[32m   1603\u001b[39m         initial_check_point = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1604\u001b[39m     \u001b[38;5;66;03m# provide the initial_check_point as it will reduce the read amplification of materialization job\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaterialization_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1606\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaterialization_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdefaultArgs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1607\u001b[39m \u001b[43m        \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1608\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m -initialCheckPointString \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43minitial_check_point\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1609\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minitial_check_point\u001b[49m\n\u001b[32m   1610\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1611\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1612\u001b[39m \u001b[43m        \u001b[49m\u001b[43mawait_termination\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwait_for_job\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1613\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1615\u001b[39m \u001b[38;5;66;03m# wait for online ingestion\u001b[39;00m\n\u001b[32m   1616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_group.online_enabled \u001b[38;5;129;01mand\u001b[39;00m offline_write_options.get(\n\u001b[32m   1617\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwait_for_online_ingestion\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1618\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mlfs/lib/python3.11/site-packages/hopsworks_common/usage.py:246\u001b[39m, in \u001b[36mmethod_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    245\u001b[39m     exception = e\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mlfs/lib/python3.11/site-packages/hopsworks_common/usage.py:242\u001b[39m, in \u001b[36mmethod_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    239\u001b[39m exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# Call the original method\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mlfs/lib/python3.11/site-packages/hopsworks_common/job.py:186\u001b[39m, in \u001b[36mJob.run\u001b[39m\u001b[34m(self, args, await_termination)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    183\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mJob started successfully, you can follow the progress at \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexecution.get_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    184\u001b[39m )\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m await_termination:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_until_finished\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m execution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mlfs/lib/python3.11/site-packages/hopsworks_common/engine/execution_engine.py:155\u001b[39m, in \u001b[36mExecutionEngine.wait_until_finished\u001b[39m\u001b[34m(self, job, execution, timeout)\u001b[39m\n\u001b[32m    153\u001b[39m     await_time -= MAX_LAG\n\u001b[32m    154\u001b[39m     time.sleep(MAX_LAG)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     updated_execution = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execution_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     log_aggregation_files_exist = \u001b[38;5;28mself\u001b[39m._dataset_api.exists(\n\u001b[32m    158\u001b[39m         updated_execution.stdout_path\n\u001b[32m    159\u001b[39m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_api.exists(updated_execution.stderr_path)\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m log_aggregation_files_exist_already:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mlfs/lib/python3.11/site-packages/hopsworks_common/core/execution_api.py:42\u001b[39m, in \u001b[36mExecutionApi._get\u001b[39m\u001b[34m(self, job, id)\u001b[39m\n\u001b[32m     31\u001b[39m path_params = [\n\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mproject\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     33\u001b[39m     _client._project_id,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[38;5;28mid\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n\u001b[32m     40\u001b[39m headers = {\u001b[33m\"\u001b[39m\u001b[33mcontent-type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m execution.Execution.from_response_json(\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m, job\n\u001b[32m     43\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mlfs/lib/python3.11/site-packages/hopsworks_common/decorators.py:48\u001b[39m, in \u001b[36mconnected.<locals>.if_connected\u001b[39m\u001b[34m(inst, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst._connected:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mlfs/lib/python3.11/site-packages/hopsworks_common/client/base.py:186\u001b[39m, in \u001b[36mClient._send_request\u001b[39m\u001b[34m(self, method, path_params, query_params, headers, data, stream, files, with_base_path_params)\u001b[39m\n\u001b[32m    181\u001b[39m     response = \u001b[38;5;28mself\u001b[39m._retry_token_expired(\n\u001b[32m    182\u001b[39m         request, stream, \u001b[38;5;28mself\u001b[39m.TOKEN_EXPIRED_RETRY_INTERVAL, \u001b[32m1\u001b[39m\n\u001b[32m    183\u001b[39m     )\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code // \u001b[32m100\u001b[39m != \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.RestAPIError(url, response)\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[31mRestAPIError\u001b[39m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/1286345/jobs/weather_1_offline_fg_materialization/executions/3828158). Server response: \nHTTP code: 404, HTTP reason: Not Found, body: b'{\"errorCode\":130009,\"usrMsg\":\"jobId:weather_1_offline_fg_materialization\",\"errorMsg\":\"Job not found.\"}', error code: 130009, error msg: Job not found., user msg: jobId:weather_1_offline_fg_materialization"
     ]
    }
   ],
   "source": [
    "# Insert new data\n",
    "weather_fg.insert(daily_df, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1666e5",
   "metadata": {},
   "source": [
    "### Add lagged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_pm25 = fs.get_or_create_feature_group(\n",
    "    name='lagged_pm25',\n",
    "    description='Lagged PM2.5 measurements',\n",
    "    version=1,\n",
    "    primary_key=['city', 'date'],\n",
    "    event_time=\"date\",\n",
    "    expectation_suite=None\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed0bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get historical PM2.5 data (EXCLUDING today to avoid data leakage)\n",
    "# These features should be computed from past data only, not today's target value\n",
    "aq_history = air_quality_fg.filter(\n",
    "    (air_quality_fg.city == city) &\n",
    "    (air_quality_fg.date >= today - timedelta(days=30)) &\n",
    "    (air_quality_fg.date < today)  # Exclude today!\n",
    ").read().sort_values('date')\n",
    "\n",
    "windows = [1, 7, 14, 21, 30]\n",
    "\n",
    "if aq_history.empty or len(aq_history) < 2:\n",
    "    # No history yet, create empty features\n",
    "    df_feat = pd.DataFrame({\n",
    "        'city': [city],\n",
    "        'date': [pd.to_datetime(today)]  # Convert date to timestamp\n",
    "    })\n",
    "    for w in windows:\n",
    "        df_feat[f'pm25_change_{w}d'] = np.nan\n",
    "        df_feat[f'pm25_std_{w}d'] = np.nan\n",
    "    # Convert NaN columns to float32 (float) to match schema\n",
    "    for w in windows:\n",
    "        df_feat[f'pm25_change_{w}d'] = df_feat[f'pm25_change_{w}d'].astype('float32')\n",
    "        df_feat[f'pm25_std_{w}d'] = df_feat[f'pm25_std_{w}d'].astype('float32')\n",
    "else:\n",
    "    # Compute features from historical data only (NO data leakage)\n",
    "    # pct_change(periods=w) looks BACKWARD: compares value to w periods ago\n",
    "    # rolling(window=w) uses the last w values\n",
    "    # We take .iloc[-1] to get the most recent historical value\n",
    "    \n",
    "    # --- PERCENT CHANGE FEATURES (from historical data) ---\n",
    "    pct_change_features = {\n",
    "        f\"pm25_change_{w}d\": aq_history[\"pm25\"].pct_change(periods=w).shift(1).iloc[-1] if len(aq_history) > w else np.nan\n",
    "        for w in windows\n",
    "    }\n",
    "    \n",
    "    # --- ROLLING STD FEATURES (from historical data) ---\n",
    "    std_features = {\n",
    "        f\"pm25_std_{w}d\": aq_history[\"pm25\"].rolling(window=w, min_periods=1).std().shift(1).iloc[-1] if len(aq_history) >= 1 else np.nan\n",
    "        for w in windows\n",
    "    }\n",
    "    \n",
    "    # --- COMBINE INTO ONE FEATURES DATAFRAME ---\n",
    "    df_feat = pd.DataFrame({**pct_change_features, **std_features}, index=[0])\n",
    "    df_feat['city'] = city\n",
    "    df_feat[\"date\"] = pd.to_datetime(today)  # Convert date to timestamp\n",
    "    \n",
    "    # Convert all numeric columns to float32 (float) to match schema\n",
    "    for col in df_feat.columns:\n",
    "        if col not in ['city', 'date']:\n",
    "            # Ensure all numeric columns are float32 (float) to match feature group schema\n",
    "            if pd.api.types.is_numeric_dtype(df_feat[col]):\n",
    "                df_feat[col] = df_feat[col].astype('float32')\n",
    "\n",
    "lagged_pm25.insert(df_feat, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e9e2d",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">‚è≠Ô∏è **Next:** Part 03: Training Pipeline\n",
    " </span> \n",
    "\n",
    "In the following notebook you will read from a feature group and create training dataset within the feature store\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
