{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "802ec6a1-b608-40a5-931f-5e7dfb2d7046",
   "metadata": {},
   "source": [
    "# Step 1: Real-Time Feature Computation\n",
    "\n",
    "This notebook is part of a demo showcasing a real-time fraud detection pipeline, utilizing Feldera for feature computation and Hopsworks as the feature store.\n",
    "\n",
    "![Real-time feature engineering pipeline using Feldera and Hosworks](./architecture.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bc94df-de0a-4faa-93b1-c98cf2948b9e",
   "metadata": {},
   "source": [
    "## Step 1.1. Create Hopsworks feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2037b50-de20-43f4-b8cc-c67196f920f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-20 20:06:31,181 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-10-20 20:06:31,186 INFO: Initializing external client\n",
      "2025-10-20 20:06:31,187 INFO: Base URL: https://stagingmain.devnet.hops.works:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-20 20:06:31,934 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://stagingmain.devnet.hops.works:443/p/119\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "import hsfs\n",
    "from hsfs.feature import Feature\n",
    "import json\n",
    "import datetime\n",
    "from feldera import FelderaClient, PipelineBuilder\n",
    "\n",
    "client = FelderaClient(\"http://localhost:8080\")\n",
    "\n",
    "project = hopsworks.login()\n",
    "hostname = project.get_url().removeprefix(\"https://\").split(\":\", 1)[0]\n",
    "\n",
    "kafka_api = project.get_kafka_api()\n",
    "\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "name = \"cc_trans_aggs_fg\"\n",
    "kafka_topic = f\"{project.name}_onlinefs\"\n",
    "aggs_topic=f\"{project.name}_{name}\"\n",
    "\n",
    "cc_trans_fg = fs.get_feature_group(name=\"credit_card_transactions\", version=1)\n",
    "card_details_fg = fs.get_feature_group(name=\"card_details\", version=1)\n",
    "\n",
    "\n",
    "# WINDOWED - frequency of transactions and other metrics in the span of a few hours, modeled as hopping window aggregates.\n",
    "windowed_fg = fs.get_or_create_feature_group(\n",
    "    name=name,\n",
    "    primary_key=[\"cc_num\"],\n",
    "    online_enabled=True,\n",
    "    version=1,\n",
    "    event_time=\"event_time\",\n",
    "    # topic_name=aggs_topic,    \n",
    "    stream=True,\n",
    "    features=[        \n",
    "        Feature(\"cc_num\", type=\"string\"),\n",
    "        Feature(\"account_id\", type=\"string\"),\n",
    "        Feature(\"bank_id\", type=\"string\"),\n",
    "        Feature(\"num_trans_last_10_mins\", type=\"bigint\"),\n",
    "        Feature(\"sum_trans_last_10_mins\", type=\"double\"),\n",
    "        Feature(\"num_trans_last_hour\", type=\"bigint\"),\n",
    "        Feature(\"sum_trans_last_hour\", type=\"double\"),\n",
    "        Feature(\"num_trans_last_day\", type=\"bigint\"),\n",
    "        Feature(\"sum_trans_last_day\", type=\"double\"),\n",
    "        Feature(\"num_trans_last_week\", type=\"bigint\"),\n",
    "        Feature(\"sum_trans_last_week\", type=\"double\"),\n",
    "        Feature(\"prev_card_present\", type=\"boolean\"),\n",
    "        Feature(\"prev_ip_transaction\", type=\"string\"),\n",
    "        Feature(\"prev_ts_transaction\", type=\"timestamp\"),\n",
    "        Feature(\"event_time\", type=\"timestamp\"),\n",
    "    ],    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31aca82c-d492-4074-b611-87904d98384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = hsfs.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6e1f45e-fb58-4325-98db-5380b818a586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/stagingmain.devnet.hops.works/jim/client_key.pem\n",
      "/tmp/stagingmain.devnet.hops.works/jim/client_cert.pem\n"
     ]
    }
   ],
   "source": [
    "path = client.get_instance()._get_client_key_path()\n",
    "cert = client.get_instance()._get_client_cert_path()\n",
    "print(path)\n",
    "print(cert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426b45a-51c9-49eb-abfc-10d29aff370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    windowed_fg.save()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38444377-ffc4-4746-89d5-1ced6fdc7053",
   "metadata": {},
   "source": [
    "## Load certs in Feldera Container\n",
    "\n",
    "Feldera expects the certs to be in /tmp/HOPSWORKS_HOST/HOPSWORKS_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40fc35be-7e0a-4ea2-a554-0816eac55cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container_id is 7e208e223696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['docker', 'exec', '7e208e223696', 'bash', '-c', 'rm -f /tmp/stagingmain.devnet.hops.works && ln -s /opt/stagingmain.devnet.hops.works/stagingmain.devnet.hops.works /tmp/stagingmain.devnet.hops.works'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Get container ID\n",
    "container_id = subprocess.check_output(\n",
    "    [\"docker\", \"ps\", \"--filter\", \"ancestor=ghcr.io/feldera/pipeline-manager:latest\", \"-q\"],\n",
    "    text=True\n",
    ").strip()\n",
    "\n",
    "print(f\"container_id is {container_id}\")\n",
    "# Run the command inside the container\n",
    "subprocess.run([\n",
    "    \"docker\", \"exec\", container_id,\n",
    "    \"bash\", \"-c\",\n",
    "    f\"rm -f /tmp/{hostname} && ln -s /opt/{hostname}/{hostname} /tmp/{hostname}\"\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44b4322a-1cea-4fd6-8ca9-02f0126456f6",
   "metadata": {},
   "source": [
    "## Step 1.2. Create Feldera pipeline\n",
    "\n",
    "We build a Feldera pipeline to transform raw transaction and profile data into features. In Feldera, feature groups are modeled as SQL views. Thus, we create a SQL program with two input tables (TRANSACTIONS and PROFILES), and two output views, one for each feature group.\n",
    "\n",
    "![Feldera pipeline](./feldera_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb8e951-69e3-4ddc-8e61-7c8ddbba3be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create SQL program parameterized by source and sink connnector configurations.\n",
    "\n",
    "\n",
    "\n",
    "def build_sql(\n",
    "    transaction_source_config: str, card_details_source_config: str, fs_sink_config: str\n",
    ") -> str:\n",
    "    return f\"\"\"\n",
    "\n",
    "    CREATE TABLE credit_card_transactions (\n",
    "        t_id BIGINT,\n",
    "        merchant_id VARCHAR,\n",
    "        ts TIMESTAMP,\n",
    "        cc_num VARCHAR,\n",
    "        amount DOUBLE,\n",
    "        ip_address VARCHAR,\n",
    "        card_present BOOLEAN\n",
    "    ) WITH (\n",
    "        'connectors' = '[{transaction_source_config}]'\n",
    "    );\n",
    "\n",
    "    CREATE MATERIALIZED VIEW rolling_aggregates AS\n",
    "    SELECT\n",
    "        t.cc_num, \n",
    "        t.ts AS event_time, \n",
    "        t.ip_address,\n",
    "        t.card_present,\n",
    "        SUM(COALESCE(amount, 0)) OVER window_10_minute AS sum_trans_last_10_mins,\n",
    "        COUNT(amount) OVER window_10_minute AS num_trans_last_10_mins,\n",
    "        SUM(COALESCE(amount, 0)) OVER window_1_hour AS sum_trans_last_hour,\n",
    "        COUNT(amount) OVER window_1_hour AS num_trans_last_hour,\n",
    "        SUM(COALESCE(amount, 0)) OVER window_1_day AS sum_trans_last_day,\n",
    "        COUNT(amount) OVER window_1_day AS num_trans_last_day,\n",
    "        SUM(COALESCE(amount, 0)) OVER window_7_day AS sum_trans_last_week,\n",
    "        COUNT(amount) OVER window_7_day AS num_trans_last_week\n",
    "    FROM\n",
    "         credit_card_transactions AS t\n",
    "    WINDOW\n",
    "        window_10_minute AS (\n",
    "            PARTITION BY cc_num\n",
    "            ORDER BY ts\n",
    "            RANGE BETWEEN INTERVAL '10' MINUTE PRECEDING AND CURRENT ROW\n",
    "        ),\n",
    "        window_1_hour AS (\n",
    "            PARTITION BY cc_num\n",
    "            ORDER BY ts\n",
    "            RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW\n",
    "        ),\n",
    "        window_1_day AS (\n",
    "            PARTITION BY cc_num\n",
    "            ORDER BY ts\n",
    "            RANGE BETWEEN INTERVAL '1' DAY PRECEDING AND CURRENT ROW\n",
    "        ),\n",
    "        window_7_day AS (\n",
    "            PARTITION BY cc_num\n",
    "            ORDER BY ts\n",
    "            RANGE BETWEEN INTERVAL '7' DAY PRECEDING AND CURRENT ROW\n",
    "        )\n",
    "    ;\n",
    "    \n",
    "    CREATE TABLE card_details (\n",
    "        cc_num VARCHAR NOT NULL,\n",
    "        account_id VARCHAR NOT NULL,\n",
    "        bank_id VARCHAR NOT NULL,\n",
    "        cc_expiry_date TIMESTAMP,\n",
    "        issue_date TIMESTAMP,\n",
    "        card_type VARCHAR,\n",
    "        status VARCHAR,\n",
    "        last_modified TIMESTAMP\n",
    "    ) WITH (\n",
    "        'connectors' = '[{card_details_source_config}]'\n",
    "    );\n",
    "\n",
    "\n",
    "    CREATE MATERIALIZED VIEW cc_trans_card AS\n",
    "    SELECT\n",
    "        ra.*,\n",
    "        cd.account_id,\n",
    "        cd.bank_id\n",
    "    FROM rolling_aggregates AS ra\n",
    "    LEFT ASOF JOIN card_details AS cd\n",
    "        MATCH_CONDITION (ra.event_time >= cd.last_modified)\n",
    "        ON ra.cc_num = cd.cc_num\n",
    "    ;\n",
    "    \n",
    "    CREATE MATERIALIZED VIEW lagged_trans AS\n",
    "    SELECT\n",
    "        ctc.*,\n",
    "        LAG(event_time) OVER \n",
    "          (PARTITION BY cc_num ORDER BY event_time ASC) AS prev_ts_transaction,\n",
    "        LAG(ip_address) OVER \n",
    "          (PARTITION BY cc_num ORDER BY ip_address ASC) AS prev_ip_transaction,\n",
    "        LAG(card_present) OVER \n",
    "          (PARTITION BY cc_num ORDER BY card_present ASC) AS prev_card_present\n",
    "    FROM cc_trans_card AS ctc;\n",
    "        \n",
    "    CREATE VIEW cc_trans_aggs_fg\n",
    "    WITH (\n",
    "        'connectors' = '[{fs_sink_config}]'\n",
    "    ) \n",
    "    AS \n",
    "        SELECT\n",
    "            cc_num,\n",
    "            event_time,\n",
    "            account_id,\n",
    "            bank_id,\n",
    "            sum_trans_last_10_mins,\n",
    "            num_trans_last_10_mins,\n",
    "            sum_trans_last_hour,\n",
    "            num_trans_last_hour,\n",
    "            sum_trans_last_day,\n",
    "            num_trans_last_day,\n",
    "            sum_trans_last_week,\n",
    "            num_trans_last_week,\n",
    "            prev_ts_transaction, \n",
    "            prev_ip_transaction,\n",
    "            prev_card_present\n",
    "        FROM lagged_trans\n",
    "    ;\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9d834-6e5b-434e-b158-e2e6888e7839",
   "metadata": {},
   "source": [
    "### Connect Kafka sources and sinks\n",
    "\n",
    "We use the Kafka topic created during the data prep step as the input for the TRANSACTIONS table. The output views are also connected to the Hopsworks feature store via Kafka. Hopsworks ingests data from Kafka using the Avro format, so we configure Feldera Kafka connectors with Avro schemas generated by Hopsworks for each feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5663a3-e5b3-46d6-9143-a4165ee31993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_consumer_kafka_config(kafka_config: dict, fg):\n",
    "    return kafka_config | {\n",
    "        \"topic\": fg._online_topic_name,\n",
    "        \"start_from\": \"earliest\",\n",
    "#     \"security.protocol\": \"SSL\",\n",
    "#     \"ssl.ca.pem\": \"-----BEGIN CERTIFICATE-----TOPSECRET0\\n-----END CERTIFICATE-----\\n\",\n",
    "#     \"ssl.key.pem\": \"-----BEGIN CERTIFICATE-----TOPSECRET1\\n-----END CERTIFICATE-----\\n\",\n",
    "#     \"ssl.certificate.pem\": \"-----BEGIN CERTIFICATE-----TOPSECRET2\\n-----END CERTIFICATE-----\\n\"\n",
    "        \n",
    "    }\n",
    "\n",
    "# \"config\": {\n",
    "#     \"topic\": \"book-fair-sales\",\n",
    "#     \"start_from\": \"earliest\",\n",
    "#     \"bootstrap.servers\": \"example.com:9092\",\n",
    "#     \"security.protocol\": \"SSL\",\n",
    "#     \"ssl.ca.pem\": \"-----BEGIN CERTIFICATE-----TOPSECRET0\\n-----END CERTIFICATE-----\\n\",\n",
    "#     \"ssl.key.pem\": \"-----BEGIN CERTIFICATE-----TOPSECRET1\\n-----END CERTIFICATE-----\\n\",\n",
    "#     \"ssl.certificate.pem\": \"-----BEGIN CERTIFICATE-----TOPSECRET2\\n-----END CERTIFICATE-----\\n\"\n",
    "# }\n",
    "    \n",
    "\n",
    "def create_producer_kafka_config(kafka_config: dict, fg, project):\n",
    "    return kafka_config | {\n",
    "        # \"topic\": f\"{project.name}_onlinefs\",\n",
    "        \"topic\": fg._online_topic_name,\n",
    "        \"auto.offset.reset\": \"earliest\",\n",
    "        \"headers\": [\n",
    "            {\n",
    "                \"key\": \"projectId\",\n",
    "                \"value\": str(project.id),\n",
    "            },\n",
    "            {\n",
    "                \"key\": \"featureGroupId\",\n",
    "                \"value\": str(fg.id),\n",
    "            },\n",
    "            {\n",
    "                \"key\": \"subjectId\",\n",
    "                \"value\": str(fg.subject[\"id\"]),\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "kafka_config = kafka_api.get_default_config()\n",
    "\n",
    "fs_sink_config = json.dumps(\n",
    "    {\n",
    "        \"transport\": {\n",
    "            \"name\": \"kafka_output\",\n",
    "            \"config\": create_producer_kafka_config(kafka_config, windowed_fg, project),\n",
    "        },\n",
    "        \"format\": {\n",
    "            \"name\": \"avro\",\n",
    "            \"config\": {\"schema\": windowed_fg.avro_schema, \"skip_schema_id\": True},\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "transaction_source_config = json.dumps(\n",
    "    {\n",
    "        \"transport\": {\n",
    "            \"name\": \"kafka_input\",\n",
    "            \"config\": create_consumer_kafka_config(kafka_config, cc_trans_fg),\n",
    "        },\n",
    "        \"format\": {\n",
    "            \"name\": \"avro\",\n",
    "            \"config\": {\"schema\": cc_trans_fg.avro_schema, \"skip_schema_id\": True},\n",
    "        },\n",
    "    }\n",
    ")\n",
    "card_details_source_config = json.dumps(\n",
    "    {\n",
    "        \"transport\": {\n",
    "            \"name\": \"kafka_input\",\n",
    "            \"config\": create_consumer_kafka_config(kafka_config, card_details_fg),\n",
    "        },\n",
    "        \"format\": {\n",
    "            \"name\": \"avro\",\n",
    "            \"config\": {\"schema\": card_details_fg.avro_schema, \"skip_schema_id\": True},\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "sql = build_sql(transaction_source_config, card_details_source_config, fs_sink_config)\n",
    "pipeline = PipelineBuilder(client, name=\"hopsworks_kafka3\", sql=sql).create_or_replace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d661e44-fd7a-4627-a6c5-ac48198611a8",
   "metadata": {},
   "source": [
    "## Step 1.3. Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fece25e-7c9e-4967-bc9b-489af045e999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the Feldera pipeline.\n",
    "# Read profile data from the feature store and write it to the `PROFILE` table.\n",
    "pipeline.start()\n",
    "\n",
    "import time\n",
    "time.sleep(900)\n",
    "# pipeline.stop(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c9fba-a739-4f21-8440-f6ba60e1c553",
   "metadata": {},
   "source": [
    "## Schedule materialization to the offline store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e627f6d-bd5c-4022-8b4b-efaf3e9a304b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: cc_trans_aggs_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://stagingmain.devnet.hops.works:443/p/119/jobs/named/cc_trans_aggs_fg_1_offline_fg_materialization/executions\n",
      "2025-10-20 18:33:01,384 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2025-10-20 18:33:04,482 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2025-10-20 18:35:12,978 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2025-10-20 18:35:13,079 INFO: Waiting for log aggregation to finish.\n",
      "2025-10-20 18:35:21,433 INFO: Execution finished successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Execution('SUCCEEDED', 'FINISHED', '2025-10-20T16:32:53.000Z', '-op offline_fg_materialization -path hdfs:///Projects/jim/Resources/jobs/cc_trans_aggs_fg_1_offline_fg_materialization/config_1760970810909')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "windowed_fg.materialization_job.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13bc89-1b47-4e07-bec4-2eb5b3685d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_fg.materialization_job.schedule(\n",
    "    cron_expression=\"0 0 3 * * ? *\",\n",
    "    start_time=datetime.datetime.now(tz=datetime.timezone.utc),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe08d3-169c-40bd-9809-7d7c4d575e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_options={\n",
    "#     \"wait_for_online_ingestion\":\"false\",\n",
    "#     \"wait_for_job\":\"false\",\n",
    "#     \"hoodie.streamer.kafka.source.maxEvents\":\"50000000\",\n",
    "# \"hoodie.deltastreamer.source.kafka.auto.offset.reset\" : \"earliest\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8369a7-c885-43b5-a9de-56641a1881f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "kafka_api = project.get_kafka_api()\n",
    "for topic_name in [topic.name for topic in kafka_api.get_topics()]:\n",
    "    print(f\"Found topic: {topic_name}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c3e57-8aeb-461b-a20d-543bc36e29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = kafka_api.get_topic(\"dowlingj_cc_trans_aggs_fg\")\n",
    "topic.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
