{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "802ec6a1-b608-40a5-931f-5e7dfb2d7046",
   "metadata": {},
   "source": [
    "# Step 1: Real-Time Feature Computation\n",
    "\n",
    "This notebook is part of a demo showcasing a real-time fraud detection pipeline, utilizing Feldera for feature computation and Hopsworks as the feature store.\n",
    "\n",
    "![Real-time feature engineering pipeline using Feldera and Hosworks](./architecture.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bc94df-de0a-4faa-93b1-c98cf2948b9e",
   "metadata": {},
   "source": [
    "## Step 1.1. Create Hopsworks feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2037b50-de20-43f4-b8cc-c67196f920f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-10 23:22:07,815 INFO: Initializing external client\n",
      "2025-11-10 23:22:07,817 INFO: Base URL: https://stagingmain.devnet.hops.works:443\n",
      "2025-11-10 23:22:08,700 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://stagingmain.devnet.hops.works:443/p/122\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "import hsfs\n",
    "from hsfs.feature import Feature\n",
    "import json\n",
    "import datetime\n",
    "from feldera import FelderaClient, PipelineBuilder\n",
    "\n",
    "client = FelderaClient(\"http://localhost:8080\")\n",
    "\n",
    "project = hopsworks.login(engine=\"python\")\n",
    "hostname = project.get_url().removeprefix(\"https://\").split(\":\", 1)[0]\n",
    "\n",
    "kafka_api = project.get_kafka_api()\n",
    "\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "name = \"cc_trans_aggs_fg\"\n",
    "kafka_topic = f\"{project.name}_onlinefs\"\n",
    "aggs_topic=f\"{project.name}_{name}\"\n",
    "\n",
    "cc_trans_fg = fs.get_feature_group(name=\"credit_card_transactions\", version=1)\n",
    "card_details_fg = fs.get_feature_group(name=\"card_details\", version=1)\n",
    "\n",
    "\n",
    "# WINDOWED - frequency of transactions and other metrics in the span of a few hours, modeled as hopping window aggregates.\n",
    "windowed_fg = fs.get_or_create_feature_group(\n",
    "    name=name,\n",
    "    primary_key=[\"cc_num\"],\n",
    "    online_enabled=True,\n",
    "    version=1,\n",
    "    event_time=\"event_time\",\n",
    "    # topic_name=aggs_topic,    \n",
    "    stream=True,\n",
    "    features=[        \n",
    "        Feature(\"cc_num\", type=\"string\"),\n",
    "        Feature(\"account_id\", type=\"string\"),\n",
    "        Feature(\"bank_id\", type=\"string\"),\n",
    "        Feature(\"num_trans_last_10_mins\", type=\"bigint\"),\n",
    "        Feature(\"sum_trans_last_10_mins\", type=\"double\"),\n",
    "        Feature(\"num_trans_last_hour\", type=\"bigint\"),\n",
    "        Feature(\"sum_trans_last_hour\", type=\"double\"),\n",
    "        Feature(\"num_trans_last_day\", type=\"bigint\"),\n",
    "        Feature(\"sum_trans_last_day\", type=\"double\"),\n",
    "        Feature(\"num_trans_last_week\", type=\"bigint\"),\n",
    "        Feature(\"sum_trans_last_week\", type=\"double\"),\n",
    "        Feature(\"prev_card_present\", type=\"boolean\"),\n",
    "        Feature(\"prev_ip_transaction\", type=\"string\"),\n",
    "        Feature(\"prev_ts_transaction\", type=\"timestamp\"),\n",
    "        Feature(\"event_time\", type=\"timestamp\"),\n",
    "    ],    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4426b45a-51c9-49eb-abfc-10d29aff370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata operation error: (url: https://stagingmain.devnet.hops.works/hopsworks-api/api/project/122/featurestores/70/featuregroups). Server response: \n",
      "HTTP code: 400, HTTP reason: Bad Request, body: b'{\"errorCode\":270089,\"usrMsg\":\"project: james, featurestoreId: 70\",\"errorMsg\":\"The feature group you are trying to create does already exist.\"}', error code: 270089, error msg: The feature group you are trying to create does already exist., user msg: project: james, featurestoreId: 70\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    windowed_fg.save()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38444377-ffc4-4746-89d5-1ced6fdc7053",
   "metadata": {},
   "source": [
    "## Load certs in Feldera Container\n",
    "\n",
    "Feldera expects the certs to be in /tmp/HOPSWORKS_HOST/HOPSWORKS_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40fc35be-7e0a-4ea2-a554-0816eac55cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container_id is 6517d68e7e51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['docker', 'exec', '6517d68e7e51', 'bash', '-c', 'rm -f /tmp/stagingmain.devnet.hops.works && ln -s /opt/stagingmain.devnet.hops.works/stagingmain.devnet.hops.works /tmp/stagingmain.devnet.hops.works'], returncode=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Get container ID\n",
    "container_id = subprocess.check_output(\n",
    "    [\"docker\", \"ps\", \"--filter\", \"ancestor=ghcr.io/feldera/pipeline-manager:latest\", \"-q\"],\n",
    "    text=True\n",
    ").strip()\n",
    "\n",
    "print(f\"container_id is {container_id}\")\n",
    "# Run the command inside the container\n",
    "subprocess.run([\n",
    "    \"docker\", \"exec\", container_id,\n",
    "    \"bash\", \"-c\",\n",
    "    f\"rm -f /tmp/{hostname} && ln -s /opt/{hostname}/{hostname} /tmp/{hostname}\"\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44b4322a-1cea-4fd6-8ca9-02f0126456f6",
   "metadata": {},
   "source": [
    "## Step 1.2. Create Feldera pipeline\n",
    "\n",
    "We build a Feldera pipeline to transform raw transaction and profile data into features. In Feldera, feature groups are modeled as SQL views. Thus, we create a SQL program with two input tables (TRANSACTIONS and PROFILES), and two output views, one for each feature group.\n",
    "\n",
    "![Feldera pipeline](./feldera_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb8e951-69e3-4ddc-8e61-7c8ddbba3be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create SQL program parameterized by source and sink connnector configurations.\n",
    "\n",
    "\n",
    "\n",
    "def build_sql(\n",
    "    transaction_source_config: str, card_details_source_config: str, fs_sink_config: str\n",
    ") -> str:\n",
    "    return f\"\"\"\n",
    "\n",
    "    CREATE TABLE credit_card_transactions (\n",
    "        t_id BIGINT,\n",
    "        merchant_id VARCHAR,\n",
    "        ts TIMESTAMP,\n",
    "        cc_num VARCHAR,\n",
    "        amount DOUBLE,\n",
    "        ip_address VARCHAR,\n",
    "        card_present BOOLEAN\n",
    "    ) WITH (\n",
    "        'connectors' = '[{transaction_source_config}]'\n",
    "    );\n",
    "\n",
    "    CREATE MATERIALIZED VIEW rolling_aggregates AS\n",
    "    SELECT\n",
    "        t.cc_num, \n",
    "        t.ts AS event_time, \n",
    "        t.ip_address,\n",
    "        t.card_present,\n",
    "        SUM(COALESCE(amount, 0)) OVER window_10_minute AS sum_trans_last_10_mins,\n",
    "        COUNT(amount) OVER window_10_minute AS num_trans_last_10_mins,\n",
    "        SUM(COALESCE(amount, 0)) OVER window_1_hour AS sum_trans_last_hour,\n",
    "        COUNT(amount) OVER window_1_hour AS num_trans_last_hour,\n",
    "        SUM(COALESCE(amount, 0)) OVER window_1_day AS sum_trans_last_day,\n",
    "        COUNT(amount) OVER window_1_day AS num_trans_last_day,\n",
    "        SUM(COALESCE(amount, 0)) OVER window_7_day AS sum_trans_last_week,\n",
    "        COUNT(amount) OVER window_7_day AS num_trans_last_week\n",
    "    FROM\n",
    "         credit_card_transactions AS t\n",
    "    WINDOW\n",
    "        window_10_minute AS (\n",
    "            PARTITION BY cc_num\n",
    "            ORDER BY ts\n",
    "            RANGE BETWEEN INTERVAL '10' MINUTE PRECEDING AND CURRENT ROW\n",
    "        ),\n",
    "        window_1_hour AS (\n",
    "            PARTITION BY cc_num\n",
    "            ORDER BY ts\n",
    "            RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW\n",
    "        ),\n",
    "        window_1_day AS (\n",
    "            PARTITION BY cc_num\n",
    "            ORDER BY ts\n",
    "            RANGE BETWEEN INTERVAL '1' DAY PRECEDING AND CURRENT ROW\n",
    "        ),\n",
    "        window_7_day AS (\n",
    "            PARTITION BY cc_num\n",
    "            ORDER BY ts\n",
    "            RANGE BETWEEN INTERVAL '7' DAY PRECEDING AND CURRENT ROW\n",
    "        )\n",
    "    ;\n",
    "    \n",
    "    CREATE TABLE card_details (\n",
    "        cc_num VARCHAR NOT NULL,\n",
    "        account_id VARCHAR NOT NULL,\n",
    "        bank_id VARCHAR NOT NULL,\n",
    "        cc_expiry_date TIMESTAMP,\n",
    "        issue_date TIMESTAMP,\n",
    "        card_type VARCHAR,\n",
    "        status VARCHAR,\n",
    "        last_modified TIMESTAMP\n",
    "    ) WITH (\n",
    "        'connectors' = '[{card_details_source_config}]'\n",
    "    );\n",
    "\n",
    "\n",
    "    CREATE MATERIALIZED VIEW cc_trans_card AS\n",
    "    SELECT\n",
    "        ra.*,\n",
    "        cd.account_id,\n",
    "        cd.bank_id\n",
    "    FROM rolling_aggregates AS ra\n",
    "    LEFT ASOF JOIN card_details AS cd\n",
    "        MATCH_CONDITION (ra.event_time >= cd.last_modified)\n",
    "        ON ra.cc_num = cd.cc_num\n",
    "    ;\n",
    "    \n",
    "    CREATE MATERIALIZED VIEW lagged_trans AS\n",
    "    SELECT\n",
    "        ctc.*,\n",
    "        LAG(event_time) OVER \n",
    "          (PARTITION BY cc_num ORDER BY event_time ASC) AS prev_ts_transaction,\n",
    "        LAG(ip_address) OVER \n",
    "          (PARTITION BY cc_num ORDER BY ip_address ASC) AS prev_ip_transaction,\n",
    "        LAG(card_present) OVER \n",
    "          (PARTITION BY cc_num ORDER BY card_present ASC) AS prev_card_present\n",
    "    FROM cc_trans_card AS ctc;\n",
    "        \n",
    "    CREATE VIEW cc_trans_aggs_fg\n",
    "    WITH (\n",
    "        'connectors' = '[{fs_sink_config}]'\n",
    "    ) \n",
    "    AS \n",
    "        SELECT\n",
    "            cc_num,\n",
    "            event_time,\n",
    "            account_id,\n",
    "            bank_id,\n",
    "            sum_trans_last_10_mins,\n",
    "            num_trans_last_10_mins,\n",
    "            sum_trans_last_hour,\n",
    "            num_trans_last_hour,\n",
    "            sum_trans_last_day,\n",
    "            num_trans_last_day,\n",
    "            sum_trans_last_week,\n",
    "            num_trans_last_week,\n",
    "            prev_ts_transaction, \n",
    "            prev_ip_transaction,\n",
    "            prev_card_present\n",
    "        FROM lagged_trans\n",
    "    ;\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9d834-6e5b-434e-b158-e2e6888e7839",
   "metadata": {},
   "source": [
    "### Connect Kafka sources and sinks\n",
    "\n",
    "We use the Kafka topic created during the data prep step as the input for the TRANSACTIONS table. The output views are also connected to the Hopsworks feature store via Kafka. Hopsworks ingests data from Kafka using the Avro format, so we configure Feldera Kafka connectors with Avro schemas generated by Hopsworks for each feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b5663a3-e5b3-46d6-9143-a4165ee31993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_consumer_kafka_config(kafka_config: dict, fg):\n",
    "    return kafka_config | {\n",
    "        \"topic\": fg._online_topic_name,\n",
    "        \"start_from\": \"earliest\",\n",
    "#     \"security.protocol\": \"SSL\",\n",
    "#     \"ssl.ca.pem\": \"-----BEGIN CERTIFICATE-----TOPSECRET0\\n-----END CERTIFICATE-----\\n\",\n",
    "#     \"ssl.key.pem\": \"-----BEGIN CERTIFICATE-----TOPSECRET1\\n-----END CERTIFICATE-----\\n\",\n",
    "#     \"ssl.certificate.pem\": \"-----BEGIN CERTIFICATE-----TOPSECRET2\\n-----END CERTIFICATE-----\\n\"\n",
    "        \n",
    "    }\n",
    "\n",
    "# \"config\": {\n",
    "#     \"topic\": \"book-fair-sales\",\n",
    "#     \"start_from\": \"earliest\",\n",
    "#     \"bootstrap.servers\": \"example.com:9092\",\n",
    "#     \"security.protocol\": \"SSL\",\n",
    "#     \"ssl.ca.pem\": \"-----BEGIN CERTIFICATE-----TOPSECRET0\\n-----END CERTIFICATE-----\\n\",\n",
    "#     \"ssl.key.pem\": \"-----BEGIN CERTIFICATE-----TOPSECRET1\\n-----END CERTIFICATE-----\\n\",\n",
    "#     \"ssl.certificate.pem\": \"-----BEGIN CERTIFICATE-----TOPSECRET2\\n-----END CERTIFICATE-----\\n\"\n",
    "# }\n",
    "    \n",
    "\n",
    "def create_producer_kafka_config(kafka_config: dict, fg, project):\n",
    "    return kafka_config | {\n",
    "        # \"topic\": f\"{project.name}_onlinefs\",\n",
    "        \"topic\": fg._online_topic_name,\n",
    "        \"auto.offset.reset\": \"earliest\",\n",
    "        \"headers\": [\n",
    "            {\n",
    "                \"key\": \"projectId\",\n",
    "                \"value\": str(project.id),\n",
    "            },\n",
    "            {\n",
    "                \"key\": \"featureGroupId\",\n",
    "                \"value\": str(fg.id),\n",
    "            },\n",
    "            {\n",
    "                \"key\": \"subjectId\",\n",
    "                \"value\": str(fg.subject[\"id\"]),\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "kafka_config = kafka_api.get_default_config()\n",
    "\n",
    "fs_sink_config = json.dumps(\n",
    "    {\n",
    "        \"transport\": {\n",
    "            \"name\": \"kafka_output\",\n",
    "            \"config\": create_producer_kafka_config(kafka_config, windowed_fg, project),\n",
    "        },\n",
    "        \"format\": {\n",
    "            \"name\": \"avro\",\n",
    "            \"config\": {\"schema\": windowed_fg.avro_schema, \"skip_schema_id\": True},\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "transaction_source_config = json.dumps(\n",
    "    {\n",
    "        \"transport\": {\n",
    "            \"name\": \"kafka_input\",\n",
    "            \"config\": create_consumer_kafka_config(kafka_config, cc_trans_fg),\n",
    "        },\n",
    "        \"format\": {\n",
    "            \"name\": \"avro\",\n",
    "            \"config\": {\"schema\": cc_trans_fg.avro_schema, \"skip_schema_id\": True},\n",
    "        },\n",
    "    }\n",
    ")\n",
    "card_details_source_config = json.dumps(\n",
    "    {\n",
    "        \"transport\": {\n",
    "            \"name\": \"kafka_input\",\n",
    "            \"config\": create_consumer_kafka_config(kafka_config, card_details_fg),\n",
    "        },\n",
    "        \"format\": {\n",
    "            \"name\": \"avro\",\n",
    "            \"config\": {\"schema\": card_details_fg.avro_schema, \"skip_schema_id\": True},\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "sql = build_sql(transaction_source_config, card_details_source_config, fs_sink_config)\n",
    "pipeline = PipelineBuilder(client, name=\"hopsworks_kafka3\", sql=sql).create_or_replace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d661e44-fd7a-4627-a6c5-ac48198611a8",
   "metadata": {},
   "source": [
    "## Step 1.3. Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fece25e-7c9e-4967-bc9b-489af045e999",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown value 'Stopped' for enum PipelineStatus",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start the Feldera pipeline.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Read profile data from the feature store and write it to the `PROFILE` table.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      6\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m900\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/feldera/pipeline.py:357\u001b[0m, in \u001b[0;36mPipeline.start\u001b[0;34m(self, timeout_s)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout_s: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    .. _start:\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m    :raises RuntimeError: If the pipeline is not in SHUTDOWN state.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__failed_check\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m     status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus()\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m PipelineStatus\u001b[38;5;241m.\u001b[39mSHUTDOWN:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/feldera/pipeline.py:329\u001b[0m, in \u001b[0;36mPipeline.__failed_check\u001b[0;34m(self, next)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__failed_check\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mnext\u001b[39m):\n\u001b[1;32m    325\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m    Checks if the pipeline is in FAILED state and raises an error if it is.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    :meta private:\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m PipelineStatus\u001b[38;5;241m.\u001b[39mFAILED:\n\u001b[1;32m    331\u001b[0m         deployment_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mget_pipeline(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\u001b[38;5;241m.\u001b[39mdeployment_error\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/feldera/pipeline.py:64\u001b[0m, in \u001b[0;36mPipeline.status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPipelineStatus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeployment_status\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m FelderaAPIError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m err\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/feldera/enums.py:190\u001b[0m, in \u001b[0;36mPipelineStatus.from_str\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m member\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m value\u001b[38;5;241m.\u001b[39mlower():\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m member\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for enum \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPipelineStatus\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown value 'Stopped' for enum PipelineStatus"
     ]
    }
   ],
   "source": [
    "# Start the Feldera pipeline.\n",
    "# Read profile data from the feature store and write it to the `PROFILE` table.\n",
    "pipeline.start()\n",
    "\n",
    "import time\n",
    "time.sleep(900)\n",
    "# pipeline.stop(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c9fba-a739-4f21-8440-f6ba60e1c553",
   "metadata": {},
   "source": [
    "## Schedule materialization to the offline store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "253916a7-f576-4512-96ed-0c7cdfff5bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: feldera\n",
      "Version: 0.41.0\n",
      "Summary: The feldera python client\n",
      "Home-page: https://www.feldera.com\n",
      "Author: \n",
      "Author-email: Abhinav <abhinav.gyawali@feldera.com>\n",
      "License: MIT\n",
      "Location: /home/jdowling/.local/lib/python3.10/site-packages\n",
      "Requires: numpy, pandas, pretty-errors, requests, ruff, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show feldera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e627f6d-bd5c-4022-8b4b-efaf3e9a304b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: cc_trans_aggs_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://stagingmain.devnet.hops.works:443/p/122/jobs/named/cc_trans_aggs_fg_1_offline_fg_materialization/executions\n",
      "2025-11-10 23:36:11,888 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2025-11-10 23:36:14,988 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2025-11-10 23:38:56,446 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2025-11-10 23:38:56,554 INFO: Waiting for log aggregation to finish.\n",
      "2025-11-10 23:39:04,878 INFO: Execution finished successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Execution('SUCCEEDED', 'FINISHED', '2025-11-10T22:36:04.000Z', '-op offline_fg_materialization -path hdfs:///Projects/james/Resources/jobs/cc_trans_aggs_fg_1_offline_fg_materialization/config_1762813125196')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "windowed_fg.materialization_job.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d13bc89-1b47-4e07-bec4-2eb5b3685d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hopsworks_common.job_schedule.JobSchedule at 0x7523152cf970>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windowed_fg.materialization_job.schedule(\n",
    "    cron_expression=\"0 0 3 * * ? *\",\n",
    "    start_time=datetime.datetime.now(tz=datetime.timezone.utc),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe08d3-169c-40bd-9809-7d7c4d575e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_options={\n",
    "#     \"wait_for_online_ingestion\":\"false\",\n",
    "#     \"wait_for_job\":\"false\",\n",
    "#     \"hoodie.streamer.kafka.source.maxEvents\":\"50000000\",\n",
    "# \"hoodie.deltastreamer.source.kafka.auto.offset.reset\" : \"earliest\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8369a7-c885-43b5-a9de-56641a1881f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "kafka_api = project.get_kafka_api()\n",
    "for topic_name in [topic.name for topic in kafka_api.get_topics()]:\n",
    "    print(f\"Found topic: {topic_name}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c3e57-8aeb-461b-a20d-543bc36e29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = kafka_api.get_topic(\"dowlingj_cc_trans_aggs_fg\")\n",
    "topic.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
