{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "802ec6a1-b608-40a5-931f-5e7dfb2d7046",
   "metadata": {},
   "source": [
    "# Step 1: Real-Time Feature Computation\n",
    "\n",
    "This notebook is part of a demo showcasing a real-time fraud detection pipeline, utilizing Feldera for feature computation and Hopsworks as the feature store.\n",
    "\n",
    "![Real-time feature engineering pipeline using Feldera and Hosworks](./architecture.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bc94df-de0a-4faa-93b1-c98cf2948b9e",
   "metadata": {},
   "source": [
    "## Step 1.1. Create Hopsworks feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "736d8b3f-d661-4413-ab5b-9fa8e07f0162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root dir: /home/jdowling/Projects/mlfs-book\n",
      "HopsworksSettings initialized!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "root_dir = Path().absolute()\n",
    "# Strip ~/notebooks/ccfraud from PYTHON_PATH if notebook started in one of these subdirectories\n",
    "if root_dir.parts[-1:] == ('notebooks',):\n",
    "    root_dir = Path(*root_dir.parts[:-1])\n",
    "    sys.path.append(str(root_dir))\n",
    "if root_dir.parts[-1:] == ('ccfraud',):\n",
    "    root_dir = Path(*root_dir.parts[:-1])\n",
    "    sys.path.append(str(root_dir))\n",
    "root_dir = str(root_dir) \n",
    "\n",
    "print(f\"Root dir: {root_dir}\")\n",
    "\n",
    "# Set the environment variables from the file <root_dir>/.env\n",
    "from mlfs import config\n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2037b50-de20-43f4-b8cc-c67196f920f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-29 12:23:51,944 WARNING: Feldera client is on version 0.212.0 while server is at 0.143.0. There could be incompatibilities.\n",
      "2025-12-29 12:23:51,946 INFO: Initializing external client\n",
      "2025-12-29 12:23:51,948 INFO: Base URL: https://eu-west.cloud.hopsworks.ai:443\n",
      "2025-12-29 12:23:52,676 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://eu-west.cloud.hopsworks.ai:443/p/120\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "import hsfs\n",
    "from hsfs.feature import Feature\n",
    "import json\n",
    "import datetime\n",
    "from feldera import FelderaClient, PipelineBuilder\n",
    "\n",
    "client = FelderaClient(\"http://localhost:8080\")\n",
    "\n",
    "project = hopsworks.login()\n",
    "hostname = project.get_url().removeprefix(\"https://\").split(\":\", 1)[0]\n",
    "\n",
    "kafka_api = project.get_kafka_api()\n",
    "\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "name = \"cc_trans_aggs_fg\"\n",
    "kafka_topic = f\"{project.name}_onlinefs\"\n",
    "aggs_topic=f\"{project.name}_{name}_onlinefs\"\n",
    "\n",
    "cc_trans_fg = fs.get_feature_group(name=\"credit_card_transactions\", version=1)\n",
    "card_details_fg = fs.get_feature_group(name=\"card_details\", version=1)\n",
    "\n",
    "\n",
    "# WINDOWED - frequency of transactions and other metrics in the span of a few hours, modeled as hopping window aggregates.\n",
    "windowed_fg = fs.get_or_create_feature_group(\n",
    "    name=name,\n",
    "    primary_key=[\"cc_num\"],\n",
    "    online_enabled=True,\n",
    "    version=1,\n",
    "    event_time=\"event_time\",\n",
    "    stream=True,\n",
    "    features=[        \n",
    "        Feature(\"cc_num\", type=\"string\"),\n",
    "        Feature(\"account_id\", type=\"bigint\"),\n",
    "        Feature(\"bank_id\", type=\"bigint\"),\n",
    "        Feature(\"num_trans_last_10_mins\", type=\"bigint\"),\n",
    "        Feature(\"sum_trans_last_10_mins\", type=\"double\"),\n",
    "        Feature(\"num_trans_last_hour\", type=\"bigint\"),\n",
    "        Feature(\"sum_trans_last_hour\", type=\"double\"),\n",
    "        Feature(\"num_trans_last_day\", type=\"bigint\"),\n",
    "        Feature(\"sum_trans_last_day\", type=\"double\"),\n",
    "        Feature(\"num_trans_last_week\", type=\"bigint\"),\n",
    "        Feature(\"sum_trans_last_week\", type=\"double\"),\n",
    "        Feature(\"prev_card_present\", type=\"boolean\"),\n",
    "        Feature(\"prev_ip_transaction\", type=\"string\"),\n",
    "        Feature(\"prev_ts_transaction\", type=\"timestamp\"),\n",
    "        Feature(\"event_time\", type=\"timestamp\"),\n",
    "    ],    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4426b45a-51c9-49eb-abfc-10d29aff370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://eu-west.cloud.hopsworks.ai:443/p/120/fs/68/fg/3236\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    windowed_fg.save()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653325c0-7645-4670-b08f-b9ce0531e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka_api = project.get_kafka_api()\n",
    "\n",
    "# kafka_topics = kafka_api.get_topics()\n",
    "# for topic in kafka_topics:\n",
    "#     print(f\"{topic.name}\")\n",
    "#     if topic.name == \"dowlingj_credit_card_transactions_onlinefs\" or topic.name==\"dowlingj_cc_trans_aggs_fg\":\n",
    "#         topic.delete()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38444377-ffc4-4746-89d5-1ced6fdc7053",
   "metadata": {},
   "source": [
    "## Load certs in Feldera Container\n",
    "\n",
    "Feldera expects the certs to be in /tmp/HOPSWORKS_HOST/HOPSWORKS_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40fc35be-7e0a-4ea2-a554-0816eac55cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container_id is a0c8263b7714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['docker', 'exec', 'a0c8263b7714', 'bash', '-c', 'rm -f /tmp/eu-west.cloud.hopsworks.ai && ln -s /opt/eu-west.cloud.hopsworks.ai/eu-west.cloud.hopsworks.ai /tmp/eu-west.cloud.hopsworks.ai'], returncode=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Get container ID\n",
    "container_id = subprocess.check_output(\n",
    "    [\"docker\", \"ps\", \"--filter\", \"ancestor=ghcr.io/feldera/pipeline-manager:latest\", \"-q\"],\n",
    "    text=True\n",
    ").strip()\n",
    "\n",
    "print(f\"container_id is {container_id}\")\n",
    "# Run the command inside the container\n",
    "subprocess.run([\n",
    "    \"docker\", \"exec\", container_id,\n",
    "    \"bash\", \"-c\",\n",
    "    f\"rm -f /tmp/{hostname} && ln -s /opt/{hostname}/{hostname} /tmp/{hostname}\"\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44b4322a-1cea-4fd6-8ca9-02f0126456f6",
   "metadata": {},
   "source": [
    "## Step 1.2. Create Feldera pipeline\n",
    "\n",
    "We build a Feldera pipeline to transform raw transaction and profile data into features. In Feldera, feature groups are modeled as SQL views. Thus, we create a SQL program with two input tables (TRANSACTIONS and PROFILES), and two output views, one for each feature group.\n",
    "\n",
    "![Feldera pipeline](./feldera_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb8e951-69e3-4ddc-8e61-7c8ddbba3be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create SQL program parameterized by source and sink connnector configurations.\n",
    "\n",
    "\n",
    "\n",
    "def build_sql(\n",
    "    transaction_source_config: str, card_details_source_config: str, fs_sink_config: str\n",
    ") -> str:\n",
    "    return f\"\"\"\n",
    "\n",
    "    CREATE TABLE credit_card_transactions (\n",
    "        t_id BIGINT,\n",
    "        merchant_id BIGINT,\n",
    "        ts TIMESTAMP,\n",
    "        cc_num VARCHAR,\n",
    "        amount DOUBLE,\n",
    "        ip_address VARCHAR,\n",
    "        card_present BOOLEAN\n",
    "    ) WITH (\n",
    "        'connectors' = '[{transaction_source_config}]'\n",
    "    );\n",
    "\n",
    "    CREATE MATERIALIZED VIEW rolling_aggregates AS\n",
    "    SELECT\n",
    "        t.cc_num, \n",
    "        t.ts AS event_time, \n",
    "        t.ip_address,\n",
    "        t.card_present,\n",
    "        SUM(COALESCE(amount, 0)) OVER window_10_minute AS sum_trans_last_10_mins,\n",
    "        COUNT(amount) OVER window_10_minute AS num_trans_last_10_mins,\n",
    "        SUM(COALESCE(amount, 0)) OVER window_1_hour AS sum_trans_last_hour,\n",
    "        COUNT(amount) OVER window_1_hour AS num_trans_last_hour,\n",
    "        SUM(COALESCE(amount, 0)) OVER window_1_day AS sum_trans_last_day,\n",
    "        COUNT(amount) OVER window_1_day AS num_trans_last_day,\n",
    "        SUM(COALESCE(amount, 0)) OVER window_7_day AS sum_trans_last_week,\n",
    "        COUNT(amount) OVER window_7_day AS num_trans_last_week\n",
    "    FROM\n",
    "         credit_card_transactions AS t\n",
    "    WINDOW\n",
    "        window_10_minute AS (\n",
    "            PARTITION BY cc_num\n",
    "            ORDER BY ts\n",
    "            RANGE BETWEEN INTERVAL '10' MINUTE PRECEDING AND CURRENT ROW\n",
    "        ),\n",
    "        window_1_hour AS (\n",
    "            PARTITION BY cc_num\n",
    "            ORDER BY ts\n",
    "            RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW\n",
    "        ),\n",
    "        window_1_day AS (\n",
    "            PARTITION BY cc_num\n",
    "            ORDER BY ts\n",
    "            RANGE BETWEEN INTERVAL '1' DAY PRECEDING AND CURRENT ROW\n",
    "        ),\n",
    "        window_7_day AS (\n",
    "            PARTITION BY cc_num\n",
    "            ORDER BY ts\n",
    "            RANGE BETWEEN INTERVAL '7' DAY PRECEDING AND CURRENT ROW\n",
    "        )\n",
    "    ;\n",
    "    \n",
    "    CREATE TABLE card_details (\n",
    "        card_id BIGINT,\n",
    "        cc_num VARCHAR NOT NULL,\n",
    "        account_id BIGINT NOT NULL,\n",
    "        bank_id BIGINT NOT NULL,\n",
    "        cc_expiry_date TIMESTAMP,\n",
    "        issue_date TIMESTAMP,\n",
    "        card_type VARCHAR,\n",
    "        status VARCHAR,\n",
    "        last_modified TIMESTAMP\n",
    "    ) WITH (\n",
    "        'connectors' = '[{card_details_source_config}]'\n",
    "    );\n",
    "\n",
    "\n",
    "    CREATE MATERIALIZED VIEW cc_trans_card AS\n",
    "    SELECT\n",
    "        ra.*,\n",
    "        cd.account_id,\n",
    "        cd.bank_id\n",
    "    FROM rolling_aggregates AS ra\n",
    "    LEFT ASOF JOIN card_details AS cd\n",
    "        MATCH_CONDITION (ra.event_time >= cd.last_modified)\n",
    "        ON ra.cc_num = cd.cc_num\n",
    "    ;\n",
    "    \n",
    "    CREATE MATERIALIZED VIEW lagged_trans AS\n",
    "    SELECT\n",
    "        ctc.*,\n",
    "        LAG(event_time) OVER \n",
    "          (PARTITION BY cc_num ORDER BY event_time ASC) AS prev_ts_transaction,\n",
    "        LAG(ip_address) OVER \n",
    "          (PARTITION BY cc_num ORDER BY ip_address ASC) AS prev_ip_transaction,\n",
    "        LAG(card_present) OVER \n",
    "          (PARTITION BY cc_num ORDER BY card_present ASC) AS prev_card_present\n",
    "    FROM cc_trans_card AS ctc;\n",
    "        \n",
    "    CREATE VIEW cc_trans_aggs_fg\n",
    "    WITH (\n",
    "        'connectors' = '[{fs_sink_config}]'\n",
    "    ) \n",
    "    AS \n",
    "        SELECT\n",
    "            cc_num,\n",
    "            event_time,\n",
    "            account_id,\n",
    "            bank_id,\n",
    "            sum_trans_last_10_mins,\n",
    "            num_trans_last_10_mins,\n",
    "            sum_trans_last_hour,\n",
    "            num_trans_last_hour,\n",
    "            sum_trans_last_day,\n",
    "            num_trans_last_day,\n",
    "            sum_trans_last_week,\n",
    "            num_trans_last_week,\n",
    "            prev_ts_transaction, \n",
    "            prev_ip_transaction,\n",
    "            prev_card_present\n",
    "        FROM lagged_trans\n",
    "    ;\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9d834-6e5b-434e-b158-e2e6888e7839",
   "metadata": {},
   "source": [
    "### Connect Kafka sources and sinks\n",
    "\n",
    "We use the Kafka topic created during the data prep step as the input for the TRANSACTIONS table. The output views are also connected to the Hopsworks feature store via Kafka. Hopsworks ingests data from Kafka using the Avro format, so we configure Feldera Kafka connectors with Avro schemas generated by Hopsworks for each feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b5663a3-e5b3-46d6-9143-a4165ee31993",
   "metadata": {},
   "outputs": [
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://eu-west.cloud.hopsworks.ai/hopsworks-api/api/project/120/featurestores/68/kafka/subjects/credit_card_transactions_1/versions/latest). Server response: \nHTTP code: 404, HTTP reason: Not Found, body: b'{\"error_code\":40401,\"message\":\"Subject not found\"}', error code: , error msg: , user msg: ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRestAPIError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     29\u001b[39m kafka_config = kafka_api.get_default_config()\n\u001b[32m     31\u001b[39m fs_sink_config = json.dumps(\n\u001b[32m     32\u001b[39m     {\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtransport\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m     }\n\u001b[32m     42\u001b[39m )\n\u001b[32m     44\u001b[39m transaction_source_config = json.dumps(\n\u001b[32m     45\u001b[39m     {\n\u001b[32m     46\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtransport\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     47\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mkafka_input\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     48\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m: create_consumer_kafka_config(kafka_config, cc_trans_fg),\n\u001b[32m     49\u001b[39m         },\n\u001b[32m     50\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     51\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mavro\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mschema\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mcc_trans_fg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mavro_schema\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mskip_schema_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[32m     53\u001b[39m         },\n\u001b[32m     54\u001b[39m     }\n\u001b[32m     55\u001b[39m )\n\u001b[32m     57\u001b[39m card_details_source_config = json.dumps(\n\u001b[32m     58\u001b[39m     {\n\u001b[32m     59\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtransport\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m     }\n\u001b[32m     68\u001b[39m )\n\u001b[32m     70\u001b[39m sql = build_sql(transaction_source_config, card_details_source_config, fs_sink_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mlfs-book/ccfraud/.venv/lib/python3.11/site-packages/hsfs/feature_group.py:2361\u001b[39m, in \u001b[36mFeatureGroupBase.avro_schema\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2358\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m   2359\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mavro_schema\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m   2360\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Avro schema representation of the feature group.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubject\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mschema\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mlfs-book/ccfraud/.venv/lib/python3.11/site-packages/hsfs/feature_group.py:2355\u001b[39m, in \u001b[36mFeatureGroupBase.subject\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2352\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Subject of the feature group.\"\"\"\u001b[39;00m\n\u001b[32m   2353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._subject \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2354\u001b[39m     \u001b[38;5;66;03m# cache the schema\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2355\u001b[39m     \u001b[38;5;28mself\u001b[39m._subject = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feature_group_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_subject\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._subject\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mlfs-book/ccfraud/.venv/lib/python3.11/site-packages/hsfs/core/feature_group_base_engine.py:194\u001b[39m, in \u001b[36mFeatureGroupBaseEngine.get_subject\u001b[39m\u001b[34m(self, feature_group)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_subject\u001b[39m(\u001b[38;5;28mself\u001b[39m, feature_group):\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_kafka_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_subject\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_feature_store_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_fg_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mlfs-book/ccfraud/.venv/lib/python3.11/site-packages/hopsworks_common/usage.py:236\u001b[39m, in \u001b[36mmethod_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    234\u001b[39m     \u001b[38;5;66;03m# Disable usage AFTER import hsfs, return function itself\u001b[39;00m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_enabled:\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m     start_time = time.perf_counter()\n\u001b[32m    239\u001b[39m     exception = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mlfs-book/ccfraud/.venv/lib/python3.11/site-packages/hopsworks_common/core/kafka_api.py:388\u001b[39m, in \u001b[36mKafkaApi.get_subject\u001b[39m\u001b[34m(self, feature_store_id, subject, version)\u001b[39m\n\u001b[32m    376\u001b[39m path_params = [\n\u001b[32m    377\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mproject\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    378\u001b[39m     _client._project_id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    385\u001b[39m     version,\n\u001b[32m    386\u001b[39m ]\n\u001b[32m    387\u001b[39m headers = {\u001b[33m\"\u001b[39m\u001b[33mcontent-type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mlfs-book/ccfraud/.venv/lib/python3.11/site-packages/hopsworks_common/decorators.py:48\u001b[39m, in \u001b[36mconnected.<locals>.if_connected\u001b[39m\u001b[34m(inst, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst._connected:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mlfs-book/ccfraud/.venv/lib/python3.11/site-packages/hopsworks_common/client/base.py:186\u001b[39m, in \u001b[36mClient._send_request\u001b[39m\u001b[34m(self, method, path_params, query_params, headers, data, stream, files, with_base_path_params)\u001b[39m\n\u001b[32m    181\u001b[39m     response = \u001b[38;5;28mself\u001b[39m._retry_token_expired(\n\u001b[32m    182\u001b[39m         request, stream, \u001b[38;5;28mself\u001b[39m.TOKEN_EXPIRED_RETRY_INTERVAL, \u001b[32m1\u001b[39m\n\u001b[32m    183\u001b[39m     )\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code // \u001b[32m100\u001b[39m != \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.RestAPIError(url, response)\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[31mRestAPIError\u001b[39m: Metadata operation error: (url: https://eu-west.cloud.hopsworks.ai/hopsworks-api/api/project/120/featurestores/68/kafka/subjects/credit_card_transactions_1/versions/latest). Server response: \nHTTP code: 404, HTTP reason: Not Found, body: b'{\"error_code\":40401,\"message\":\"Subject not found\"}', error code: , error msg: , user msg: "
     ]
    }
   ],
   "source": [
    "def create_consumer_kafka_config(kafka_config: dict, fg):\n",
    "    return kafka_config | {\n",
    "        \"topic\": fg._online_topic_name,\n",
    "        \"start_from\": \"earliest\",        \n",
    "    }\n",
    "    \n",
    "\n",
    "def create_producer_kafka_config(kafka_config: dict, fg, project):\n",
    "    return kafka_config | {\n",
    "        \"topic\": f\"{project.name}_onlinefs\",\n",
    "        \"auto.offset.reset\": \"earliest\",\n",
    "        \"headers\": [\n",
    "            {\n",
    "                \"key\": \"projectId\",\n",
    "                \"value\": str(project.id),\n",
    "            },\n",
    "            {\n",
    "                \"key\": \"featureGroupId\",\n",
    "                \"value\": str(fg.id),\n",
    "            },\n",
    "            {\n",
    "                \"key\": \"subjectId\",\n",
    "                \"value\": str(fg.subject[\"id\"]),\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "kafka_config = kafka_api.get_default_config()\n",
    "\n",
    "fs_sink_config = json.dumps(\n",
    "    {\n",
    "        \"transport\": {\n",
    "            \"name\": \"kafka_output\",\n",
    "            \"config\": create_producer_kafka_config(kafka_config, windowed_fg, project),\n",
    "        },\n",
    "        \"format\": {\n",
    "            \"name\": \"avro\",\n",
    "            \"config\": {\"schema\": windowed_fg.avro_schema, \"skip_schema_id\": True},\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "transaction_source_config = json.dumps(\n",
    "    {\n",
    "        \"transport\": {\n",
    "            \"name\": \"kafka_input\",\n",
    "            \"config\": create_consumer_kafka_config(kafka_config, cc_trans_fg),\n",
    "        },\n",
    "        \"format\": {\n",
    "            \"name\": \"avro\",\n",
    "            \"config\": {\"schema\": cc_trans_fg.avro_schema, \"skip_schema_id\": True},\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "card_details_source_config = json.dumps(\n",
    "    {\n",
    "        \"transport\": {\n",
    "            \"name\": \"kafka_input\",\n",
    "            \"config\": create_consumer_kafka_config(kafka_config, card_details_fg),\n",
    "        },\n",
    "        \"format\": {\n",
    "            \"name\": \"avro\",\n",
    "            \"config\": {\"schema\": card_details_fg.avro_schema, \"skip_schema_id\": True},\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "sql = build_sql(transaction_source_config, card_details_source_config, fs_sink_config)\n",
    "pipeline = PipelineBuilder(client, name=\"hopsworks_kafka\", sql=sql).create_or_replace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d661e44-fd7a-4627-a6c5-ac48198611a8",
   "metadata": {},
   "source": [
    "## Step 1.3. Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fece25e-7c9e-4967-bc9b-489af045e999",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Start the Feldera pipeline.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Read profile data from the feature store and write it to the `PROFILE` table.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mpipeline\u001b[49m.start()\n",
      "\u001b[31mNameError\u001b[39m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Start the Feldera pipeline.\n",
    "# Read profile data from the feature store and write it to the `PROFILE` table.\n",
    "pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe20fe5-4d25-4b5b-be22-c465661727b7",
   "metadata": {},
   "source": [
    "## Schedule periodic materialization to the offline store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13bc89-1b47-4e07-bec4-2eb5b3685d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# wait 900s until data has been written before materializing to the offline store\n",
    "# time.sleep(900)\n",
    "# pipeline.stop(force=True)\n",
    "windowed_fg.materialization_job.run()\n",
    "\n",
    "# windowed_fg.materialization_job.schedule(\n",
    "#     cron_expression=\"0 0 3 * * ? *\",\n",
    "#     start_time=datetime.datetime.now(tz=datetime.timezone.utc),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe08d3-169c-40bd-9809-7d7c4d575e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_options={\n",
    "#     \"wait_for_online_ingestion\":\"false\",\n",
    "#     \"wait_for_job\":\"false\",\n",
    "#     \"hoodie.streamer.kafka.source.maxEvents\":\"50000000\",\n",
    "# \"hoodie.deltastreamer.source.kafka.auto.offset.reset\" : \"earliest\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8369a7-c885-43b5-a9de-56641a1881f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hopsworks\n",
    "\n",
    "# project = hopsworks.login()\n",
    "# kafka_api = project.get_kafka_api()\n",
    "# for topic_name in [topic.name for topic in kafka_api.get_topics()]:\n",
    "#     print(f\"Found topic: {topic_name}\")\n",
    "# topic = kafka_api.get_topic(\"dowlingj_cc_trans_aggs_fg\")\n",
    "# topic.delete()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
