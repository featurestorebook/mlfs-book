{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "802ec6a1-b608-40a5-931f-5e7dfb2d7046",
   "metadata": {},
   "source": [
    "# Step 1: Real-Time Feature Computation\n",
    "\n",
    "This notebook is part of a demo showcasing a real-time fraud detection pipeline, utilizing Feldera for feature computation and Hopsworks as the feature store.\n",
    "\n",
    "![Real-time feature engineering pipeline using Feldera and Hosworks](./architecture.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bc94df-de0a-4faa-93b1-c98cf2948b9e",
   "metadata": {},
   "source": [
    "## Step 1.1. Create Hopsworks feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d8b3f-d661-4413-ab5b-9fa8e07f0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "root_dir = Path().absolute()\n",
    "# Strip ~/notebooks/ccfraud from PYTHON_PATH if notebook started in one of these subdirectories\n",
    "if root_dir.parts[-1:] == ('notebooks',):\n",
    "    root_dir = Path(*root_dir.parts[:-1])\n",
    "    sys.path.append(str(root_dir))\n",
    "if root_dir.parts[-1:] == ('ccfraud',):\n",
    "    root_dir = Path(*root_dir.parts[:-1])\n",
    "    sys.path.append(str(root_dir))\n",
    "root_dir = str(root_dir) \n",
    "\n",
    "print(f\"Root dir: {root_dir}\")\n",
    "\n",
    "# Set the environment variables from the file <root_dir>/.env\n",
    "from mlfs import config\n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2037b50-de20-43f4-b8cc-c67196f920f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "import hsfs\n",
    "from hsfs.feature import Feature\n",
    "import json\n",
    "import datetime\n",
    "from feldera import FelderaClient, PipelineBuilder\n",
    "\n",
    "client = FelderaClient(\"http://localhost:8080\")\n",
    "\n",
    "project = hopsworks.login()\n",
    "hostname = project.get_url().removeprefix(\"https://\").split(\":\", 1)[0]\n",
    "\n",
    "kafka_api = project.get_kafka_api()\n",
    "\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "name = \"cc_trans_aggs_fg\"\n",
    "kafka_topic = f\"{project.name}_onlinefs\"\n",
    "aggs_topic=f\"{project.name}_{name}_onlinefs\"\n",
    "\n",
    "cc_trans_fg = fs.get_feature_group(name=\"credit_card_transactions\", version=1)\n",
    "card_details_fg = fs.get_feature_group(name=\"card_details\", version=1)\n",
    "\n",
    "\n",
    "# WINDOWED - frequency of transactions and other metrics in the span of a few hours, modeled as hopping window aggregates.\n",
    "windowed_fg = fs.get_or_create_feature_group(\n",
    "    name=name,\n",
    "    primary_key=[\"cc_num\"],\n",
    "    online_enabled=True,\n",
    "    version=1,\n",
    "    event_time=\"event_time\",\n",
    "    stream=True,\n",
    "    features=[        \n",
    "        Feature(\"t_id\", type=\"bigint\"),\n",
    "        Feature(\"cc_num\", type=\"string\"),\n",
    "        Feature(\"account_id\", type=\"string\"),\n",
    "        Feature(\"bank_id\", type=\"string\"),\n",
    "        Feature(\"num_trans_last_10_mins\", type=\"bigint\"),\n",
    "        Feature(\"sum_trans_last_10_mins\", type=\"double\"),\n",
    "        Feature(\"num_trans_last_hour\", type=\"bigint\"),\n",
    "        Feature(\"sum_trans_last_hour\", type=\"double\"),\n",
    "        Feature(\"num_trans_last_day\", type=\"bigint\"),\n",
    "        Feature(\"sum_trans_last_day\", type=\"double\"),\n",
    "        Feature(\"num_trans_last_week\", type=\"bigint\"),\n",
    "        Feature(\"sum_trans_last_week\", type=\"double\"),\n",
    "        Feature(\"prev_card_present\", type=\"boolean\"),\n",
    "        Feature(\"prev_ip_transaction\", type=\"string\"),\n",
    "        Feature(\"prev_ts_transaction\", type=\"timestamp\"),\n",
    "        Feature(\"event_time\", type=\"timestamp\"),\n",
    "    ],    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426b45a-51c9-49eb-abfc-10d29aff370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    windowed_fg.save()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653325c0-7645-4670-b08f-b9ce0531e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka_api = project.get_kafka_api()\n",
    "\n",
    "# kafka_topics = kafka_api.get_topics()\n",
    "# for topic in kafka_topics:\n",
    "#     print(f\"{topic.name}\")\n",
    "#     if topic.name == \"dowlingj_credit_card_transactions_onlinefs\" or topic.name==\"dowlingj_cc_trans_aggs_fg\":\n",
    "#         topic.delete()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38444377-ffc4-4746-89d5-1ced6fdc7053",
   "metadata": {},
   "source": [
    "## Load certs in Feldera Container\n",
    "\n",
    "Feldera expects the certs to be in /tmp/HOPSWORKS_HOST/HOPSWORKS_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc35be-7e0a-4ea2-a554-0816eac55cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Get container ID\n",
    "container_id = subprocess.check_output(\n",
    "    [\"docker\", \"ps\", \"--filter\", \"ancestor=ghcr.io/feldera/pipeline-manager:latest\", \"-q\"],\n",
    "    text=True\n",
    ").strip()\n",
    "\n",
    "print(f\"container_id is {container_id}\")\n",
    "# Run the command inside the container\n",
    "subprocess.run([\n",
    "    \"docker\", \"exec\", container_id,\n",
    "    \"bash\", \"-c\",\n",
    "    f\"rm -f /tmp/{hostname} && ln -s /opt/{hostname}/{hostname} /tmp/{hostname}\"\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44b4322a-1cea-4fd6-8ca9-02f0126456f6",
   "metadata": {},
   "source": [
    "## Step 1.2. Create Feldera pipeline\n",
    "\n",
    "We build a Feldera pipeline to transform raw transaction and profile data into features. In Feldera, feature groups are modeled as SQL views. Thus, we create a SQL program with two input tables (TRANSACTIONS and PROFILES), and two output views, one for each feature group.\n",
    "\n",
    "![Feldera pipeline](./feldera_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb8e951-69e3-4ddc-8e61-7c8ddbba3be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create SQL program parameterized by source and sink connnector configurations.\n",
    "\n",
    "\n",
    "\n",
    "def build_sql(\n",
    "    transaction_source_config: str, card_details_source_config: str, fs_sink_config: str\n",
    ") -> str:\n",
    "    return f\"\"\"\n",
    "\n",
    "    CREATE TABLE credit_card_transactions (\n",
    "        t_id BIGINT,\n",
    "        merchant_id VARCHAR,\n",
    "        ts TIMESTAMP,\n",
    "        cc_num VARCHAR,\n",
    "        amount DOUBLE,\n",
    "        ip_address VARCHAR,\n",
    "        card_present BOOLEAN\n",
    "    ) WITH (\n",
    "        'connectors' = '[{transaction_source_config}]'\n",
    "    );\n",
    "\n",
    "    CREATE MATERIALIZED VIEW rolling_aggregates AS\n",
    "    SELECT\n",
    "        t.t_id,\n",
    "        t.cc_num, \n",
    "        t.ts AS event_time, \n",
    "        t.ip_address,\n",
    "        t.card_present,\n",
    "        SUM(COALESCE(amount, 0)) OVER window_10_minute AS sum_trans_last_10_mins,\n",
    "        COUNT(amount) OVER window_10_minute AS num_trans_last_10_mins,\n",
    "        SUM(COALESCE(amount, 0)) OVER window_1_hour AS sum_trans_last_hour,\n",
    "        COUNT(amount) OVER window_1_hour AS num_trans_last_hour,\n",
    "        SUM(COALESCE(amount, 0)) OVER window_1_day AS sum_trans_last_day,\n",
    "        COUNT(amount) OVER window_1_day AS num_trans_last_day,\n",
    "        SUM(COALESCE(amount, 0)) OVER window_7_day AS sum_trans_last_week,\n",
    "        COUNT(amount) OVER window_7_day AS num_trans_last_week\n",
    "    FROM\n",
    "         credit_card_transactions AS t\n",
    "    WINDOW\n",
    "        window_10_minute AS (\n",
    "            PARTITION BY cc_num\n",
    "            ORDER BY ts\n",
    "            RANGE BETWEEN INTERVAL '10' MINUTE PRECEDING AND CURRENT ROW\n",
    "        ),\n",
    "        window_1_hour AS (\n",
    "            PARTITION BY cc_num\n",
    "            ORDER BY ts\n",
    "            RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW\n",
    "        ),\n",
    "        window_1_day AS (\n",
    "            PARTITION BY cc_num\n",
    "            ORDER BY ts\n",
    "            RANGE BETWEEN INTERVAL '1' DAY PRECEDING AND CURRENT ROW\n",
    "        ),\n",
    "        window_7_day AS (\n",
    "            PARTITION BY cc_num\n",
    "            ORDER BY ts\n",
    "            RANGE BETWEEN INTERVAL '7' DAY PRECEDING AND CURRENT ROW\n",
    "        )\n",
    "    ;\n",
    "    \n",
    "    CREATE TABLE card_details (\n",
    "        card_id VARCHAR,\n",
    "        cc_num VARCHAR NOT NULL,\n",
    "        account_id VARCHAR NOT NULL,\n",
    "        bank_id VARCHAR NOT NULL,\n",
    "        cc_expiry_date TIMESTAMP,\n",
    "        issue_date TIMESTAMP,\n",
    "        card_type VARCHAR,\n",
    "        status VARCHAR,\n",
    "        last_modified TIMESTAMP\n",
    "    ) WITH (\n",
    "        'connectors' = '[{card_details_source_config}]'\n",
    "    );\n",
    "\n",
    "\n",
    "    CREATE MATERIALIZED VIEW cc_trans_card AS\n",
    "    SELECT\n",
    "        ra.*,\n",
    "        cd.account_id,\n",
    "        cd.bank_id\n",
    "    FROM rolling_aggregates AS ra\n",
    "    LEFT ASOF JOIN card_details AS cd\n",
    "        MATCH_CONDITION (ra.event_time >= cd.last_modified)\n",
    "        ON ra.cc_num = cd.cc_num\n",
    "    ;\n",
    "    \n",
    "    CREATE MATERIALIZED VIEW lagged_trans AS\n",
    "    SELECT\n",
    "        ctc.*,\n",
    "        LAG(event_time) OVER \n",
    "          (PARTITION BY cc_num ORDER BY event_time ASC) AS prev_ts_transaction,\n",
    "        LAG(ip_address) OVER \n",
    "          (PARTITION BY cc_num ORDER BY ip_address ASC) AS prev_ip_transaction,\n",
    "        LAG(card_present) OVER \n",
    "          (PARTITION BY cc_num ORDER BY card_present ASC) AS prev_card_present\n",
    "    FROM cc_trans_card AS ctc;\n",
    "        \n",
    "    CREATE VIEW cc_trans_aggs_fg\n",
    "    WITH (\n",
    "        'connectors' = '[{fs_sink_config}]'\n",
    "    ) \n",
    "    AS \n",
    "        SELECT\n",
    "            t_id,\n",
    "            cc_num,\n",
    "            event_time,\n",
    "            account_id,\n",
    "            bank_id,\n",
    "            sum_trans_last_10_mins,\n",
    "            num_trans_last_10_mins,\n",
    "            sum_trans_last_hour,\n",
    "            num_trans_last_hour,\n",
    "            sum_trans_last_day,\n",
    "            num_trans_last_day,\n",
    "            sum_trans_last_week,\n",
    "            num_trans_last_week,\n",
    "            prev_ts_transaction, \n",
    "            prev_ip_transaction,\n",
    "            prev_card_present\n",
    "        FROM lagged_trans\n",
    "    ;\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9d834-6e5b-434e-b158-e2e6888e7839",
   "metadata": {},
   "source": [
    "### Connect Kafka sources and sinks\n",
    "\n",
    "We use the Kafka topic created during the data prep step as the input for the TRANSACTIONS table. The output views are also connected to the Hopsworks feature store via Kafka. Hopsworks ingests data from Kafka using the Avro format, so we configure Feldera Kafka connectors with Avro schemas generated by Hopsworks for each feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5663a3-e5b3-46d6-9143-a4165ee31993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_consumer_kafka_config(kafka_config: dict, fg):\n",
    "    return kafka_config | {\n",
    "        \"topic\": fg._online_topic_name,\n",
    "        \"start_from\": \"earliest\",        \n",
    "    }\n",
    "    \n",
    "\n",
    "def create_producer_kafka_config(kafka_config: dict, fg, project):\n",
    "    return kafka_config | {\n",
    "        \"topic\": f\"{project.name}_onlinefs\",\n",
    "        \"auto.offset.reset\": \"earliest\",\n",
    "        \"headers\": [\n",
    "            {\n",
    "                \"key\": \"projectId\",\n",
    "                \"value\": str(project.id),\n",
    "            },\n",
    "            {\n",
    "                \"key\": \"featureGroupId\",\n",
    "                \"value\": str(fg.id),\n",
    "            },\n",
    "            {\n",
    "                \"key\": \"subjectId\",\n",
    "                \"value\": str(fg.subject[\"id\"]),\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "kafka_config = kafka_api.get_default_config()\n",
    "\n",
    "fs_sink_config = json.dumps(\n",
    "    {\n",
    "        \"transport\": {\n",
    "            \"name\": \"kafka_output\",\n",
    "            \"config\": create_producer_kafka_config(kafka_config, windowed_fg, project),\n",
    "        },\n",
    "        \"format\": {\n",
    "            \"name\": \"avro\",\n",
    "            \"config\": {\"schema\": windowed_fg.avro_schema, \"skip_schema_id\": True},\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "transaction_source_config = json.dumps(\n",
    "    {\n",
    "        \"transport\": {\n",
    "            \"name\": \"kafka_input\",\n",
    "            \"config\": create_consumer_kafka_config(kafka_config, cc_trans_fg),\n",
    "        },\n",
    "        \"format\": {\n",
    "            \"name\": \"avro\",\n",
    "            \"config\": {\"schema\": cc_trans_fg.avro_schema, \"skip_schema_id\": True},\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "card_details_source_config = json.dumps(\n",
    "    {\n",
    "        \"transport\": {\n",
    "            \"name\": \"kafka_input\",\n",
    "            \"config\": create_consumer_kafka_config(kafka_config, card_details_fg),\n",
    "        },\n",
    "        \"format\": {\n",
    "            \"name\": \"avro\",\n",
    "            \"config\": {\"schema\": card_details_fg.avro_schema, \"skip_schema_id\": True},\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "sql = build_sql(transaction_source_config, card_details_source_config, fs_sink_config)\n",
    "pipeline = PipelineBuilder(client, name=\"hopsworks_kafka\", sql=sql).create_or_replace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d661e44-fd7a-4627-a6c5-ac48198611a8",
   "metadata": {},
   "source": [
    "## Step 1.3. Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fece25e-7c9e-4967-bc9b-489af045e999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the Feldera pipeline.\n",
    "# Read profile data from the feature store and write it to the `PROFILE` table.\n",
    "pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe20fe5-4d25-4b5b-be22-c465661727b7",
   "metadata": {},
   "source": [
    "## Schedule periodic materialization to the offline store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13bc89-1b47-4e07-bec4-2eb5b3685d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# wait until data has been written before materializing to the offline store\n",
    "sleep_secs=180\n",
    "print(f\"Sleeping for {sleep_secs} secs for feldera to compute and write features before materializing to offline store\")\n",
    "time.sleep(sleep_secs)\n",
    "# pipeline.stop(force=True)\n",
    "windowed_fg.materialization_job.run()\n",
    "\n",
    "# windowed_fg.materialization_job.schedule(\n",
    "#     cron_expression=\"0 0 3 * * ? *\",\n",
    "#     start_time=datetime.datetime.now(tz=datetime.timezone.utc),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe08d3-169c-40bd-9809-7d7c4d575e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_options={\n",
    "#     \"wait_for_online_ingestion\":\"false\",\n",
    "#     \"wait_for_job\":\"false\",\n",
    "#     \"hoodie.streamer.kafka.source.maxEvents\":\"50000000\",\n",
    "# \"hoodie.deltastreamer.source.kafka.auto.offset.reset\" : \"earliest\",\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
